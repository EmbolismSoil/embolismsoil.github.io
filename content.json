{"meta":{"title":"兰溪","subtitle":"君子和而不同","description":null,"author":"兰溪","url":"https://embolismsoil.github.io","root":"/"},"pages":[],"posts":[{"title":"让机器读懂文章-pLSA模型推导及实现","slug":"让机器读懂文章-pLSA模型推导及实现","date":"2019-07-11T18:09:57.000Z","updated":"2019-07-12T18:38:01.140Z","comments":true,"path":"2019/07/12/让机器读懂文章-pLSA模型推导及实现/","link":"","permalink":"https://embolismsoil.github.io/2019/07/12/让机器读懂文章-pLSA模型推导及实现/","excerpt":"","text":"概述人类读懂文章是一个很自然的行为，当我们读完一篇《背影》的时候，我们就可以知道这篇文章在写些什么，也就是我们说获得了这篇文章的相关知识。有了这些知识，我们就可以回答一些问题，例如: 问：这篇文章写的主要内容是什么呢？ 答： 亲情、送别 问：有类似《背影》这样的文章可以推荐的吗？ 答：龙应台-《送别》 虽然上面的问答对任务对人类来说十分简单，但对于机器来说却并不容易。机器对自然语言(中文文本)的理解实际上并不是非常简单的事情，因为自然语言本身是一个高层抽象的概念，而机器只擅长处理量化的知识，例如说让机器记住向量$\\vec x=[1, 2, 3]$和$\\vec y=[4, 5, 6]$是十分容易的事情，而且可以轻易知道$\\vec x$和$\\vec y$的相似程度，这只需要计算其记录即可，于是我们对于向量来说就可以完成上面的问题2了。 让我们重新揣摩一下人类读懂文章的过程，实际上我们并不需要背熟每一个字词，而是阅读完成之后再总结出这篇文章主要在写什么，也就是文章的主题。为了让机器能理解文章，我们也需要把这些主题量化出来，形成类似$\\overrightarrow {topic}=[‘亲情’: 0.5, ‘送别’: 0.5]$的向量，这种能量化文章主题的模型，也就叫做主题模型了。 在主题模型方面前人们已经做了很多工作，并且取得了非常不错的成效，其中影响较大的是一类模型叫做隐语义模型，而这类模型里面概率隐语义分析也就是本文所述的pLSA则是应用最成功的模型之一，同样成功的模型还有隐含狄利克雷分布，也就是大名鼎鼎的LDA主题模型，不过LDA与pLSA的思想一脉相承，只不过做了贝叶斯改造而已。 pLSA模型事实上pLSA是在对我们写一篇文章的行为建模，我们先揣摩朱自清先生写《背影》的行为。首先我朱先生敲定了今天要写一篇《背影》，然后他开始构思了这篇文章的主题为：亲情、送别，并且朱先生认为这两部分的内容都几乎同等重要，也就是: $[‘亲情’: 0.5, ‘送别’: 0.5]$，朱先生开始动笔，于是当朱先生写下 我买几个橘子去，你就在此地，不要走动。 实际上是朱先生先前构思好的亲情、父子、送别这三个中心思想在影响着朱先生写下了这段话。于是在这三个中心思想的影响下，朱先生写完了《背影》里面的所有词，而我们读者所谓的理解《背影》，实际上就是从我们看到的《背影》的所有词，推断出了朱先生构思的主题: $[‘亲情’: 0.5, ‘送别’: 0.5]$。而pLSA则只是用数学化的形式描述这个过程, 这样一个形式化的过程在pLSA的眼里是这样的： 从分布$p(d_m)$上采样选择了一篇文章$d_m$ 对于文章$d_m$每一个词，从分布$p(z_k|d_m)$上采样一个生成一个主题$z_k$ 从分布$p(w_n|z_k)$上采样生成了一个词$w_n$ 这个模型可以用plate notation更加简洁地描述： 图中的阴影部分的变量$d$和$w$对应着文章和文章的所有词，表示可观测的变量，$z$是观测不到的主题，我们称之为隐变量，两个框框左下角的$N$和$M$则分别表示$N$和$M$词独立重复试验。这个图所表达的文章生成过程跟上面的文字表述是一致的。 这样写文章的模型是符合直觉的，但仔细推敲总觉得过于机械生硬，这样的机械式过程能写得出朱先生《背影》那样优秀的文章吗? 如果无限多个猴子任意敲打打字机键，最终会写出大英博物馆的所有藏书 — 爱丁顿无限猴子理论 一件小概率的事件在多次试验中必然发生，这就是为什么随机敲打键盘的猴子也能作的原因，于是上面问题答案自然是肯定的，pLSA这样合乎直觉的模型当然要比一只茫无目的敲打键盘的猴子更加具备写作天赋。 我们读者需要阅读根据文章和文章的所有内容去推断文章的主题，而pLSA眼里则是根据可观测变量$w$和可观测变量$d$去推断隐变量$z$。我们可以通过海量的文章去解算出模型中的参数，也就是上文中的$p(z_k|p_m)$和$p(w_n|z_k)$两个分布，我们称之为文章主题分布和主题词分布。 而$p(z_k|d_m)$这个分布实际上就是文章$d_m$的主题分布，也就是我们前文所说的$[‘亲情’: 0.5, ‘送别’: 0.5]$这样的文章主题，这个分布就是我们就获取到关于文章的知识，它量化说明了文章$d_m$在说什么内容。至于模型参数解算的过程，这没什么不可以理解的，正如我定义了一个$y$的产生过程过$y=ax+b$, 当我拿到足够多的样本$y_0=0, y_1=1, y_2=2,….y_n=n$之后，实际上我可以将他们组成方程组解出合理的参数$a$、$b$和$x$来。 行文至此，我们且对pLSA的求解按下不表，先来实际感受一下pLSA的作用。这里选择格林童话中的十几篇童话作为语料训练pLSA，然后分别从5个主题分布中取出的top3词语： topic-1 topic-2 topic-3 topic-4 topic-5 wrong birds morning soldier good issue fox met king gave faith horse wood castle great 可以看到pLSA是可以正确推导出来主题分布的。 pLSA的EM算法推导pLSA是一种含隐变量的生成模型，也就是概率化地描述了样本数据(文章)的生成并且包含隐藏变量的模型，对于这种模型可用MCMC或EM算法来求解。本文讲解的是pLSA的EM算法求解，这里并不打算讲解EM的具体推导，而是直接利用EM算法的结论来对pLSA模型求解，关于EM算法的内容读者可以自己网上搜罗一下资料，或者待我抽空再写一篇关于EM算法的文章。在开始推导之前，我们先假设词库大小为$j$, 每篇文章都由词库中的词$w_j$构成。然后定义模型参数: \\begin{aligned} \\theta_{mk}=p(z_k|d_m) \\\\ \\psi_{kj} = p(w_j|z_k) \\end{aligned} \\tag{1}根据EM算法的求解步骤，我们先根据plate notation写出联合分布: p(\\bf w, \\bf z, \\bf d) = \\prod_m p(d_m) \\prod_n p(w_{mn}|z_{mn})p(z_{mn}|d_m) \\tag{2}其中$d_m$表示第$m$篇文章， $w_{mn}$表示第$m$篇文章中的第$n$个词，$z_{mn}$表示第$m$篇文章中第$n$个词对应的主题。然后我们令给定模型参数下的主题后验证分布为： Q(\\bf z; \\bf \\theta, \\bf \\psi) = p(\\bf z|\\bf d, \\bf w; \\bf \\theta, \\bf \\psi) \\tag{3}于是可以启动EM算法当中的求期望步骤： \\sum_{\\bf z} {Q(\\bf z)lnp(\\bf w, \\bf z, \\bf w)} = \\sum_mlnp(d_m) \\sum_n \\sum_kq(z_{mnk})ln[p(w_{mn}|z_k)p(z_k|d_m)] \\tag{4}其中$q(z_{mnk})$表示在给定参数下的主题验分布，这里有: q(z_{mnk}) = p(z_k|d_m, w_n; \\theta_{mk}, \\psi_{kj}) = \\frac {p(d_m)\\theta_{mk}\\psi_{kn}}{\\sum_kp(d_m)\\theta_{mk}\\psi_{kn}} \\tag{5}由于文章中总会出现许多重复词，例如文章$d_m$中第1个词和第$5$个词是一样的，那么就会有$w_{m1}=w_{m5}=w_j$那么对于式子$(4)$中$\\sum_n \\sum_kq(z_{mnk})ln[p(w_{mn}|z_k)p(z_k|d_m)]$这部分，我们可以将文章$d_m$中重复出现的词对应的项合并成为$\\sum_j n_{mj}\\sum_kq(z_{mjk})ln[p(w_j|z_k)p(z_k|d_m)]$, 其中$n_{mj}$为文章$d_m$中词$w_j$出现的次数。于是我们重写式子$(4)$为： \\sum_{\\bf z} {Q(\\bf z)lnp(\\bf w, \\bf z, \\bf w)} = \\sum_mlnp(d_m) \\sum_j n_{mj}\\sum_kq(z_{mjk})ln(\\theta_{mk}\\psi_{kj}) \\tag{6}我们的目标是最大化式子$(6)$, 并且因为参数$\\bf \\theta$和$\\bf \\psi$是概率分布，所以有要约束$\\sum_k\\theta_{km}=1$和$\\sum_j{\\psi_{kj}} = 1$, 并且由于$p(d_m)$这个先验证分布可以设置为常数，这样我们去除与优化无关的常数项和增加了约束之后，就可以得到整个带约束的优化目标: \\begin{aligned} \\max \\limits_{\\theta_{mk}, \\psi_{kj}} \\quad & \\sum_m \\sum_j n_{mj}\\sum_kq(z_{mjk})ln(\\theta_{mk}\\psi_{kj}) \\\\ \\bf{s.t.} \\quad & \\sum_{k}\\theta_{mk}=1, m=1,2,3,...,M \\\\ & \\sum_{j} \\psi_{kj} = 1, k=1,2,3,...,K \\end{aligned} \\tag{7}这个带约束的优化目标直接使用拉格朗日乘子法： L(\\bf \\theta, \\bf \\psi, \\bf \\lambda, \\bf \\alpha) = \\sum_m \\sum_j n_{mj} \\sum_k q(z_{mjk})ln(\\theta_{mk}\\psi_{kj}) + \\sum_m {\\lambda_m} (1-\\sum_k\\theta_{mk}) + \\sum_k \\alpha_{k} (1-\\sum_j {\\psi_{kj}}) \\tag{8}于是可以对参数$\\theta_{mk}$求导并令其为0: \\frac{ \\partial L(\\bf \\theta, \\bf \\psi, \\bf \\lambda, \\bf \\alpha)}{\\partial \\theta_{mk}} = \\frac{ \\sum_jn_{mj}q(z_{mjk})}{\\theta_{mk}} - \\lambda_m = 0 \\\\ \\lambda_m \\theta_{mk} ={ \\sum_jn_{mj}q(z_{mjk})} \\tag{9}式子(9)左右两边对$k$求和得到: \\lambda_m \\sum_k \\theta_{mk} = \\sum_j{n_{mj}} \\sum_{z}q(z_{mjk}) \\\\ \\lambda_m = \\sum_j {n_{mj}} = N_m \\tag{10}上述式子(10)中$N_m$表示文章$d_m$的总词数，将式子$(10)$代回式(9)可以得到: \\theta_{mk} = \\frac {\\sum_j n_{mj}q(z_{mjk})}{N_m} \\tag{11}同样地我们对参数$\\psi_{kj}$故技重施: \\frac {\\partial L(\\bf \\theta, \\bf \\psi,\\bf \\lambda, \\bf \\alpha)}{\\partial \\psi_{kj}} = \\frac{\\sum_m n_{mj}q(z_{mjk})}{\\psi_{kj}} - \\alpha_k = 0 \\\\ \\alpha_k \\psi_{kj} = \\sum_m n_{mj}q(z_{mjk}) \\tag{12}式子(12)左右两边对$j$求和得到: \\alpha_k \\sum_{j} \\psi_{kj} = \\sum_m \\sum_j n_{mj} q(z_{mjk}) \\\\ \\alpha_k = \\sum_m \\sum_j n_{mj} q(z_{mjk}) \\tag{13}将式子代回$(12)$得到： \\psi_{kj} = \\frac { \\sum_m n_{mj}q(z_{mjk})}{ \\sum_m \\sum_j n_{mj} q(z_{mjk})} \\tag{14}至此，pLSA参数求解完毕。根据参数更新的规则，我们设在EM算法迭代运行的过程中，第$i$轮的参数为$\\theta_{mk}^i$和$\\psi_{kj}^i$。于是整个pLSA的EM算法可以归纳为： 随机初始化参数$\\theta_{mk}^0$和$\\psi_{kj}^0$ 开始第$i\\in[1, 2, 3…n]$轮迭代: a. 求$q(z_{mjk})=\\frac {p(d_m)\\theta_{mk}^{i-1}\\psi_{kj}^{i-1}}{\\sum_kp(d_m)\\theta_{mk}^{i-1}\\psi_{kj}^{i-1}}$ b. 更新参数 \\theta_{mk}^i = \\frac {\\sum_j n_{mj}q(z_{mjk})}{N_m} \\\\ \\psi_{kj}^i = \\frac { \\sum_m n_{mj}q(z_{mjk})}{ \\sum_m \\sum_j n_{mj} q(z_{mjk})} c. 若参数收敛，则退出迭代，否则返回a继续迭代 输出模型参数$\\bf \\theta$和$\\bf \\psi$ pLSA的实现从上边的式子来看pLSA是相对比较容易实现的，但是高效地实现还需要一些技巧。首先看式(14)的分母，存在一个二阶求和的过程，如果语料库中有1000篇文档，10000个词，那么就要进行一千万次运算，这样显然必须要用并行批量计算的方式来加速，在实现上我们会将涉及的所有运算都转换为矩阵运算，这样就可以通过成熟的GPU库来加速运算。其次再看内存消耗问题，$q(z_{mjk})$总共需要储存m*j*k个参数，如果有1000篇文档10000个词和50个主题，那么$q(z_{mjk})$将有5亿个元素，这在内存消耗上是不可接受的，在实现上我们只会在批量计算$\\theta_{mk}$和$\\psi_{kj}$参数时用到的部分$q(z_{mjk})$批量计算出来，并且一旦使用完毕立即丢弃。具体代码就不在这里贴了，完整的demo见pLSA实现 总结pLSA是概率隐语义主题模型中相对简单的一种，推导和实现都相对简单，回头看上面的算法过程，实际上只需要简单地计数迭代而已，所以pLSA非常适合在线学习。其实并非pLSA有此特点，事实上大多数生成模型都一样适合在线学习。不过pLSA的缺点也是非常明显的，pLSA将文章建模时没有考虑文章词序，也就是我们随机将一篇文章词打散，对于pLSA来说，其联合概率$p(\\bf w, \\bf z, \\bf d)$是不变的,这一点回头看式子$(2)$就知道。这意味着”谁是你爸爸”和”你爸爸是谁”这两句话在pLSA眼里看来是一样的，这种情况在短文本场景中尤其常见。但幸运的是，在长文本领域，有研表究明，汉字的序顺并不能影阅响读。不过pLSA近年来正在逐渐被更新颖复杂的LDA代替，但相对LDA来说pLSA结构简单，容易做大规模并行化，所以时至今日，pLSA在大规模文本挖掘领域依旧光耀夺目。 最后，向Thomas Hofmann先生致敬，感谢先生为我们带来如此精妙的pLSA主题模型。 参考文献[1] Probabilistic Latent Semantic Analysis[2] Tutorial on Probablistic Latent Semantic Analysis","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"https://embolismsoil.github.io/categories/自然语言处理/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://embolismsoil.github.io/tags/机器学习/"},{"name":"算法","slug":"算法","permalink":"https://embolismsoil.github.io/tags/算法/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"https://embolismsoil.github.io/tags/自然语言处理/"}]},{"title":"svm优化问题的导出","slug":"svm优化问题的导出","date":"2019-07-11T17:53:57.000Z","updated":"2019-07-12T18:42:49.577Z","comments":true,"path":"2019/07/12/svm优化问题的导出/","link":"","permalink":"https://embolismsoil.github.io/2019/07/12/svm优化问题的导出/","excerpt":"","text":"SVM - 优化问题的导出svm的想法其实非常朴素： 寻找一个超平面来将所有样本正确分开 (条件1) 并且保证超平面到两类样本的边界到超平面的距离和最大且相等 (条件2) 条件一其实就是线性可分的条件，条件二是为了保证鲁棒性，保证两类样本到超平面的距离最大，就相当于保留了判断时的裕量，这样即使数据有噪声，只要噪声不是太过于离谱，都不会产生误判，而保证两类样本边界到超平面距离相等则是为了不偏向某一方。 用图来直观地感受下上面两条约束，svm模型图示如下： 现在我们可以用数学的方式来描述这两个想法。 对于约束一，假设有一个超平面$w^Tx+b=0$可以将两类样本分开，那么对于正类样本$x_p$来说必然有$w^Tx_p+b &gt; 0$；同样地，对于负类样本$x_n$来说也会有$w^Tx_n + b &lt; 0$。如果我们让正类样本标签为$y_p=+1$而负类样本标签为$y_p=-1$，那么我们可以统一描述为 y(w^Tx+b) > 0 \\tag{1}现在我们来看条件2。 为了对条件2进行建模，我们首先要求空间中任意一点到超平面$w^Tx+b=0$的距离。现在我们假设有一点$x_1$，它到超平面的距离$\\gamma$可以写做 \\gamma=|w^Tx_1+b| \\tag{2}这个公式怎么来的呢？我们可以假设点$x_1$在超平面上的投影为$x_0$,那么从$x_1$指向$x_0$的这个向量就等于$x_1 - x_0$，同时这个向量的方向与超平面的法向量是一致的，我们把超平面化为$\\frac{w^Tx+b}{||w||} = 0$,可以得到法向量$\\frac{w}{||w||}$,所以我们可以得出 \\frac{\\gamma w}{||w||} = x_1 - x_0 \\tag{3}其中$\\gamma$就是点$x_1$到超平面的距离。式子(3)简单地做一下变形就可以得到式子(4) \\frac{\\gamma w^Tw}{||w||} = w^T(x_1 - x_0) + b - b = w^Tx_1 + b - (w^Tx_0 + b) \\tag{4}因为$x_0$是超平面$w^Tx + b = 0$上面的点，也就意味着$w^Tx_0 + b = 0$,这样如果只看距离的大小而不看方向的话，式子(4)就可以化为 \\gamma = \\frac{|w^Tx_1+b|}{||w||}好了，有了距离公式之后我们可以计算一下两类样本到超平面的边界距离之和了。假设有正类的边界样本$x_+$和负类边界样本$x_-$, 距离之和为 \\gamma_+ + \\gamma_- = \\frac{w^T(x_+ - x_-)}{||w||} \\tag{5}我们观察一下式子(5)，在给定样本之后，由于分母$||w||$的存在，消除了参数向量$w$的模长的影响, $\\frac{w}{||w||}$相当于一个方向与$w$同向的单位向量。于是我们只需要关注参数向量$w$的方向，而不需要关注其长度，从另一个角度来说，我们可以对$w$进行任意倍数的缩放而不会影响超平面。于是我们可以随意地令： \\begin{cases} \\quad w^T(x_+-x_-)=2 \\\\ \\quad w^Tx_+ + b = 1 \\\\ \\quad w^Tx_- + b = -1 \\end{cases}这并没有什么难以理解的，因为不管$w$和$b$的方向如何，我们总是可以对其进行缩放使得上述等式成立。这样由于正负类边界到超平面的距离都缩放到了$1$，式子(5)改写成 \\gamma_+ + \\gamma_- = \\frac{2}{||w||} \\tag{6}于是约束(1)也需要改写成 y(w^Tx+b) \\ge 1 \\tag{7}于是式子(6)和(7)构成了我们的优化目标与约束： 又由于最大化$\\frac{2}{||w||}$与最小化$\\frac{||w||^2}{2}$是等价的，而后者可以得到一个更规整的导数，方便后续处理，所以我们把上述的优化问题重写为: \\begin{cases} \\quad \\underset{w,b}{\\arg \\min} \\frac{||w||^2}{2} \\\\ \\quad s.t. \\quad y_i(w^Tx_i + b) \\ge 1, \\quad i = (1, 2, 3, ...m) \\end{cases} \\tag{8}其实我们重新审视一下式子(8), 约束条件未免太过严格，实际上大多数数据都很少存在这样完美的线性可分的条件，于是我们打算放宽一点限制: 不严格要求对每个样本都满足约束条件$y_i(w^Tx_i + b) \\ge 1$，而是允许一定程度地违反该约束，并且违反程度通过$max(0, 1 - y_i(w^Tx_i+b))$来量化并且作为一个正则化项加入到优化目标当中。于是式子(8)进一步写成 \\underset{w,b}{\\arg \\min} \\frac{||w||^2}{2} + C\\sum_{i=1}^m{max(0, 1-y_i(w^Tx_i+b))} \\tag{9}现在看一看式子(9),很遗憾，由于含有非线性且不连续的部分，导致式子(9)也难以求解，于是我们只能寻求新的方法。好在这类问题前人们已经研究过了，解决的办法称之为:松弛变量法。在最优化领域，如果我们的约束函数全为’$\\ge$’或者’$\\le$’条件时，我们希望对满足约束的样本保持原有的约束，而对那些不满足约束的样本适当放松约束(边界变得松弛了)，并且将放松的程度作为一个惩罚量加入最优化目标函数中，这样我们就可以在更大的可行域中去最优化目标函数。在这里的做法是将约束$y_i(w^Tx_i+b) \\ge 1$放松到$y_i(w^Tx_i + b) \\ge 1-\\xi_i$并且将$\\xi_i$作为一个惩罚量加入目标函数中去，根据这个想法得到的优化目标与约束为 \\begin{aligned} & \\underset{w,b,\\xi_i}{\\arg \\min} \\frac{||w||^2}{2} + C\\sum_{i=0}^m\\xi_i \\\\ s.t. & \\begin{cases} y_i(w^Tx_i + b) \\ge 1-\\xi_i, \\quad i = (1, 2, 3...,m) \\\\ \\xi_i \\ge 0, \\quad i = (i, 2, 3...,m) \\end{cases} \\end{aligned} \\tag{10}以上式子就是SVM的带约束优化目标了，解决式$(10)$的方式有很多，一般通用的做法是通过拉格朗日乘子法将带约束问题转换为无约束的最优化问题，从而能够利用梯度下降，坐标上升法等迭代算法来求解。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://embolismsoil.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://embolismsoil.github.io/tags/机器学习/"},{"name":"算法","slug":"算法","permalink":"https://embolismsoil.github.io/tags/算法/"}]},{"title":"在Qt中实现数字音频均衡器","slug":"在Qt中实现数字音频均衡器","date":"2019-07-11T17:06:56.000Z","updated":"2019-07-12T18:42:37.362Z","comments":true,"path":"2019/07/12/在Qt中实现数字音频均衡器/","link":"","permalink":"https://embolismsoil.github.io/2019/07/12/在Qt中实现数字音频均衡器/","excerpt":"","text":"Qt实现数字音频均衡器在实现音频播放器的时候，我们常常需要一个均衡器来调节各个频段的增益，就是我们平常说的调重低音。一个数字均衡器的架构通常都如图所示： 从图中可以看到，这里的数字均衡器实际上就是三个滤波器，各个滤波器分别负责不同频段的音频调节，这三个滤波器叫做滤波器组。当然一个数字滤波器组也不一定只有三个滤波器，理论上来说可以有任意多个滤波器，而且滤波器越多，能调整的也就越精细。 从物理上来说人类听觉的频率范围在0~20kHZ这个范围，于是我们定义三个频段：[0,Hz 400Hz], [400, 2000HZ], [2000Hz, 无穷]，分别为低音，中音和高音。于是我们只需要设计出来一个[0, 400Hz]的低通滤波器，一个[400, 2000Hz]的带通滤波器，以及一个[2000Hz，无穷]的高通滤波器就可以组成一个均衡器了。当然本文并不是讨论如何设计滤波器的，这是一个复杂的数学推导过程，有兴趣的可以来信探讨。另外，我已经设计好了这三个滤波器，文末会附上代码。 好了，现在我们有了思路了，来看看如何在Qt中实现这个想法。首先最重要的是提供这个滤波器组，一个滤波器组大概长这样： class EQFilterGroup { public: /*参数： 低音增益， 中音增益， 高音增益*/ EQFilterGroup(float const lowGain, float const midGain, float const highGain); /*setter and getter here*/ virtual QBuffer* filter(QAudioBuffer const&amp; buffer); } 这个类提供一个接口，这个接口输入一个音频帧(QBuffer)，然后输出滤波后的音频帧。 然后我们看一下Qt中如何播放一个音频流： QFile sourceFile; QAudioOutput *audio = new QAudioOutput(this); sourceFile.setFileName(&quot;/tmp/test.raw&quot;); sourceFile.open(QIODevice::ReadOnly); QAudioFormat format; format.setSampleRate(8000); format.setChannelCount(1); format.setSampleSize(8); format.setCodec(&quot;audio/pcm&quot;); format.setByteOrder(QAudioFormat::LittleEndian); format.setSampleType(QAudioFormat::UnSignedInt); QAudioDeviceInfo info(QAudioDeviceInfo::defaultOutputDevice()); if (!info.isFormatSupported(format)) { qWarning() &lt;&lt; &quot;Raw audio format not supported by backend, cannot play audio.&quot;; return; } audio = new QAudioOutput(format, this); audio-&gt;start(&amp;sourceFile); 其实非常简单，只需要给QAudioOutput这个组件设置好播放的音频格式参数，然后提供一个音频流就可以播放这个音频流了，在Qt里面所有的数据流都被抽象称为了QIODevice, 当然音频流也不例外，代码片段中的sourceFile就是一个音频文件数据流，它是QIODevice的子类。于是现在的问题是： 我们如何将EQFilterGroup整合到上述代码当中？如果我们能把滤波器组伪装成一个像上面代码里sourceFile的文件一样就好了，那么QAudioOutput就可以直接读取滤波器组滤波后的数据流了。这里我们可以使用适配器模式来帮助我们完成这一个伪装。根据适配器模式的做法，我们只需要继承QIODevice然后实现对应的接口并且集成EQFilterGroup的滤波功能就可以实现一个可以滤波的QIODevice 了。大概代码如下： class AudioBufferDevice : public QIODevice { Q_OBJECT public: explicit AudioBufferDevice(QAudioDecoder *decoder, EQFilterGroup* filter, QObject *parent = nullptr); virtual bool atEnd() const override; virtual qint64 bytesAvailable() const override; protected: virtual qint64 readData(char* data, qint64 size) override; virtual qint64 writeData(const char *data, qint64 maxSize); private: QAudioDecoder* _decoder; QQueue&lt;QBuffer*&gt; _queue; QQueue&lt;QAudioBuffer*&gt; _abuffer_queue; EQFilterGroup* _filter; bool _isFinished; }; 由于我们需要播放的是mp3文件，所以我们首先要通过QAudioDecoder来将mp3文件解码成音频帧，然后将音频帧输入滤波器组，滤波器组将滤波后的音频帧写入一个FIFO缓冲区内，并且通过QIODevice::readData接口向外界提供这些滤波后音频帧的数据流。当然，出于性能考虑，从QAudioDecoder解码到EQFilterGroup将滤波后数据写入缓冲池这一个过程也可以放入另一个线程中进行。 这样完成了上述的类之后，我们就可以实现一个低音炮播放器了： EQFilterGroup* filter = new EQFilterGroup(2.0, 0.5, 0.5); //放大低音2倍, 中音高音弱化为1/2 QAudioDecoder* decoder = new QAudioDecoder(this); decoder-&gt;setSourceFilename(&quot;/tmp/test.raw&quot;); QAudioFormat format; format.setSampleRate(8000); format.setChannelCount(1); format.setSampleSize(8); format.setCodec(&quot;audio/pcm&quot;); format.setByteOrder(QAudioFormat::LittleEndian); format.setSampleType(QAudioFormat::UnSignedInt); decoder-&gt;setAudioFormat(format); QIODevice *device = new AudioBufferDevice(decoder, filter); device-&gt;open(QIODevice::ReadOnly); QAudioOutput *audio = new QAudioOutput(this); QAudioDeviceInfo info(QAudioDeviceInfo::defaultOutputDevice()); if (!info.isFormatSupported(format)) { qWarning() &lt;&lt; &quot;Raw audio format not supported by backend, cannot play audio.&quot;; return; } audio = new QAudioOutput(format, this); audio-&gt;start(device); 以上就是在Qt中实现数字音频均衡器的全部啦，完整的代码在这里Qt实现数字音频均衡器","categories":[{"name":"Qt","slug":"Qt","permalink":"https://embolismsoil.github.io/categories/Qt/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://embolismsoil.github.io/tags/C/"},{"name":"Qt","slug":"Qt","permalink":"https://embolismsoil.github.io/tags/Qt/"},{"name":"音频算法","slug":"音频算法","permalink":"https://embolismsoil.github.io/tags/音频算法/"}]}]}