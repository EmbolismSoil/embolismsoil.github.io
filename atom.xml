<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>兰溪</title>
  
  <subtitle>君子和而不同</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://embolismsoil.github.io/"/>
  <updated>2019-12-15T14:07:55.955Z</updated>
  <id>https://embolismsoil.github.io/</id>
  
  <author>
    <name>兰溪</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>平均场变分推断：以混合高斯模型为例</title>
    <link href="https://embolismsoil.github.io/2019/12/15/%E5%B9%B3%E5%9D%87%E5%9C%BA%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%EF%BC%9A%E4%BB%A5%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E4%B8%BA%E4%BE%8B/"/>
    <id>https://embolismsoil.github.io/2019/12/15/平均场变分推断：以混合高斯模型为例/</id>
    <published>2019-12-15T03:15:29.000Z</published>
    <updated>2019-12-15T14:07:55.955Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h2 id="一、贝叶斯推断的工作流"><a href="#一、贝叶斯推断的工作流" class="headerlink" title="一、贝叶斯推断的工作流"></a>一、贝叶斯推断的工作流</h2><p>在贝叶斯推断方法中，工作流可以总结为：</p><ol><li>根据观察者的知识，做出合理假设，假设数据是如何被生成的</li><li>将数据的生成模型转化为数学模型</li><li>根据数据通过数学方法，求解模型参数</li><li>对新的数据做出预测</li></ol><p><img src="1576379999606.png" alt="1576379999606"></p><p>在整个pipeline中，第1点数据的生成过程，这是业务相关的，需要丰富的领域知识。第二点是连接业务领域和数学领域的桥梁，一般称之为数学建模，第3点是存粹的数学步骤，使用数学工具求解模型参数。第4步为业务应用，为业务做出预测和推断结果。</p><h2 id="二、一个业务例子"><a href="#二、一个业务例子" class="headerlink" title="二、一个业务例子"></a>二、一个业务例子</h2><p>下面以一个业务例子来实践这个pipeline。假设在我们的app中总用户数为k，并且在app的数据库中记录了用户每日使用时长的增长量，假设我们没有任何关于用户的唯一id，能观测到的只有增量值: <code>[-6.04231708, -1.64784446, ..., 1.63898137, -4.29063685, -0.87892713]</code> ，我们需要做的是为每个用户赋予一个用户id，并且在未来时刻给定某个用户使用时长增量，将这个时长增量归属到其用户id上，如此便可以建立每个用户的使用时长记录，以便在商业上分析用户行为。</p><p>首先，于是我们根据业务知识，知道用户产生时长增量记录的过程为：</p><ol><li>某个用户$c_k$ 登录系统 </li><li>用户产生一条使用时长增量记录$x_i$,</li></ol><p>然后，我们将这个过程建模为数学问题：假设登录到系统上的用户为$k’$的概率是均匀分布的，概率都为$\frac{1}{k}$，并且用户$k’$生的时长增量为一个随机变量，其符合均值为$u_k$, 标准差为$1$的分布，并且$u_{k}$自身也是符合均值为0，方差为$\sigma$的高斯分布，特别注意的是$\sigma$是一个人工设定的超参数，为领域专家根据先验知识调整的。数学化的表述如下:</p><script type="math/tex; mode=display">\begin{aligned} \mu_{k} & \sim \mathcal{N}\left(0, \sigma^{2}\right), & k &=1, \ldots, K \\ c_{i} & \sim \operatorname{Categorical}(1 / K, \ldots, 1 / K), & & i=1, \ldots, n \\ x_{i} | c_{i}, \mu & \sim \mathcal{N}\left(c_{i}^{\top} \mu, 1\right) & & i=1, \ldots, n \end{aligned}</script><p>其中$c_i$为指示向量，它指明了当前这条数据$x_i$是由哪个用户产生的，假如由用户1产生的，那么对应的指示向量为$c_i=[0,1,0,…,0,0]$，$u$为k维向量，每一维度为$u_k$, 即$u=[u_1, u_2, …, u_k]$根据上面的描述，我们可以用python代码写出整个数据产生的过程:</p><pre><code class="lang-python">def gen_data(sigma, k, n):    #获取u_k    u_k = np.random.normal(0, sigma, k)    print(u_k)    x = []    c = []    for i in range(n):        ci = random.choice(range(k))        c.append(ci)        u = u_k[ci]        x.append(np.random.normal(u, 1))    return x, u_k, c</code></pre><p>我们使用这个函数产生$(\sigma=10, k=10)$实验室数据，采样得到的$u=[-34.59, -30.27, -20.69, -19.65, -8.04, 3.0, 13.79, 14.6, 15.65, 26.56]$, 数据x分布如下图:</p><p><img src="1576417611798.png" alt="1576417611798"></p><p>于是我们的目标是根据数据x，去计算得出$u$，然后未来有数据$x’$到来时查看数据与$u_{i\in k}$的距离，选择距离最近的$i=argmin_{i} dis(u_{i \in k}, x’)$赋予该条数据作为其用户id即可。</p><h2 id="三、变分推断"><a href="#三、变分推断" class="headerlink" title="三、变分推断"></a>三、变分推断</h2><p>上述问题的思路很直接，就是根据条件概率公式，计算$u, c$的后验分布，选择后验概率最大的$u, c$即可</p><script type="math/tex; mode=display">p(u, c|x)=\frac{p(u, c, x)}{p(x)}</script><p>OK， 说干就干，首先根据第二节的数学模型，写出分子部分:</p><script type="math/tex; mode=display">p(\mathbf \mu, \mathbf{c}, \mathbf{x})=p(\mu) \prod_{i=1}^{n} p\left(c_{i}\right) p\left(x_{i} | c_{i}, \mu\right) \tag{1}</script><p>然后写出分母部分:</p><script type="math/tex; mode=display">p(\mathbf{x})=\int p(\boldsymbol{\mu}) \prod_{i=1}^{n} \sum_{c_{i}} p\left(c_{i}\right) p\left(x_{i} | c_{i}, \mu\right) \mathrm{d} \mu</script><p>这个问题在于计算$p(x)$存在一个对$u$的多维积分，在这个例子中，有10个用户即意味着10维积分，计算复杂度随着$u$的维度指数增长，在现实世界里$u$的维度小则几千几万，大则千万上亿，计算$p(x)$是现有计算机的计算能力无法完成的，变分推断的出现就是为了解决这个难题。</p><p>变分推断的思路是将该问题转换为一个最优化问题，并通过现有高效的最优化方法来进行求解。首先我们将无法观测到的隐藏变量聚拢为$z=(u, c)$，然后使用一个分布$q(z)$来逼近$p(z|x)$，两个分布之间的逼近程度用KL散度来衡量，于是我们的问题被转化成为了一个最小化$KL(q(z)||p(z|x))$的最优化问题，形式化地表述为:</p><script type="math/tex; mode=display">q(z)^*=\mathop{\arg \min}_{q(z)} KL(q(z)||p(z|x))</script><p>我们将KL散度展开为:</p><script type="math/tex; mode=display">\begin {aligned}\operatorname{KL}(q(\mathbf{z}) \| p(\mathbf{z} | \mathbf{x})) &= \mathbb{E}_{q(z)}[\log q(\mathbf{z})]-\mathbb{E}_{q(z)}[\log p(\mathbf{z} | \mathbf{x})] \\&= \mathbb{E}_{q(z)}[\log q(\mathbf{z})]-\mathbb{E}_{q(z)}[\log p(\mathbf{z,x})] +\log p(x)\end {aligned}</script><p>在最小化的过程中由于$\log p(x)$难以计算，于是退而求其次，稍作变换最小化目标为$\mathbb{E}_{q(z)}[\log q(\mathbf{z})]-\mathbb{E}_{q(z)}[\log p(\mathbf{z,x})]$, 等同于最大化其取负后的结果，形式化地表述如下:</p><script type="math/tex; mode=display">q(z)^*=\mathop{\arg \max}_{q(z)} \mathbb{E}_{q(z)}[\log p(\mathbf{z}, \mathbf{x})]-\mathbb{E}_{q(z)}[\log q(\mathbf{z})]</script><p>上述地优化目标被称之为evidence lower bound ，即ELBO, 国内有翻译为“证据下界”，也有翻译为“变分下界”。换个角度来看ELBO，其实$ELBO(q(z))=\log p(x) - KL(q(z)||p(z|x))$，因为KL散度的非负性，于是有:</p><script type="math/tex; mode=display">\log p(x) \ge ELBO(q(z))</script><p>所以本质上最大化ELBO就是在最大化$\log p(x)$，这一点在最大似然估计和最大后验估计上思路都是相似的。最后，只要我们最后选择一个参数化的$q(z)$，然后使用最优化算法即可求解原问题。</p><h2 id="四、平均场理论"><a href="#四、平均场理论" class="headerlink" title="四、平均场理论"></a>四、平均场理论</h2><p>选取合适的参数化$q(z)$可以使得求解原问题变得简单易行，当我们取$q(z)=q(z|x; \theta_t)$时，就得到了EM算法，具体展开ELBO并且迭代求解即可。但这里介绍另一种求解方法: 平均场。平均场理论并不假设具体$q(z)$的函数形式，但它假设隐变量之间相互独立:</p><script type="math/tex; mode=display">q(\mathbf{z})=\prod_{j=1}^{m} q_{j}\left(z_{j}\right)</script><p>这在很多场合下是符合直觉的，例如在上述场景中，用户使用时长的增量分布大都由自身兴趣爱好决定，两个用户之间对app的使用的时长程度并不相关。</p><p>为了求解模型，我们将平均场假设带入ELBO，首先处理第一部分:</p><script type="math/tex; mode=display">\begin {aligned}\mathbb{E}_{q(z)}[\log p(\mathbf{z}, \mathbf{x})] &= \int q(\mathbf{z}) \log p(\mathbf{z}, \mathbf{x}) d\mathbf{z} \\&= \int \prod_{i} q(z_i) \log p(\mathbf{z}, \mathbf{x}) dz_i \\&= \int q(z_j) \left[ \int \prod_{i \neq j} q(z_i) \log p(\mathbf{z}, \mathbf{x}) dz_i \right ] dz_j \\&= \int q(z_j) \mathbb{E}_{q(z_{i\neq j})} \left[ \log p(\mathbf{z}, \mathbf{x}) \right]dz_j\end {aligned}</script><p>接着处理第二部分:</p><script type="math/tex; mode=display">\begin {aligned}\mathbb{E}_{q(z)}[\log q(\mathbf{z})] &=\underset{q\left(z_{j}\right)}{\mathbb{E}}\left[\log q_{j}\left(z_{j}\right)\right]-\underset{q_{i\neq j}\left(z_{i}\right)}{\mathbb{E}}\left[\sum_{i \neq j} \log q_{i}\left(z_{i}\right)\right] \\&= \left[ \int q(z_j) \log q(z_j) dz_j \right] - \underset{q_{i\neq j}\left(z_{i}\right)}{\mathbb{E}}\left[\sum_{i \neq j} \log q_{i}\left(z_{i}\right)\right]\end {aligned}</script><p>两部分合起来得到:</p><script type="math/tex; mode=display">ELBO(q(\mathbf z))=\left[ \int q(z_j) \mathbb{E}_{q(z_{i\neq j})} \left[ \log p(\mathbf{z}, \mathbf{x}) \right]dz_j \right]  - \left[ \int q(z_j) \log q(z_j) dz_j \right] + \underset{q_{i\neq j}\left(z_{i}\right)}{\mathbb{E}}\left[\sum_{i \neq j} \log q_{i}\left(z_{i}\right)\right]</script><p>这里利用坐标上升法进行优化在每一轮迭代时，将$q(z_{i \neq j})$固定，然后优化$q(z_j)$。于是我们能把注意力集中到$q(z_j)$上来，在上面式子中$q(z_{i \neq j})$已经固定，这时候$\underset{q_{i\neq j}\left(z_{i}\right)}{\mathbb{E}}\left[\sum_{i \neq j} \log q_{i}\left(z_{i}\right)\right]=c$为常数，于是上面式子可以简化一下:</p><script type="math/tex; mode=display">ELBO(q(z_j))=\left[ \int q(z_j) \mathbb{E}_{q(z_{i\neq j})} \left[ \log p(\mathbf{z}, \mathbf{x}) \right]dz_j \right]  - \left[ \int q(z_j) \log q(z_j) dz_j \right] + c</script><p>最后我们令$\log D=\mathbb{E}_{q(z_{i\neq j})} \left[ \log p(\mathbf{z}, \mathbf{x}) \right]$, 于是得到:</p><script type="math/tex; mode=display">\begin {aligned}ELBO(q(z_j)) &= \left[ \int q(z_j) \log D dz_j \right]  - \left[ \int q(z_j) \log q(z_j) dz_j \right] + c \\&= -KL(q(z_j)||D) + c\end {aligned}</script><p>由KL散度可以知道当取$q(z_j)=D$即:</p><script type="math/tex; mode=display">q(z_j)=e^{\mathbb{E}_{q(z_{i\neq j})} \left[ \log p(\mathbf{z}, \mathbf{x}) \right]} \tag{2}</script><p>时KL散度最小为0，有$EBLO(q(z_j))=c$最大。于是我们得到了平均场变分推断算法:</p><ol><li>迭代计算每一个$q(z_j)=e^{\mathbb{E}_{q(z_{i\neq j})} \left[ \log p(\mathbf{z}, \mathbf{x}) \right]}$</li><li>计算$ELBO(q(\mathbf z))$，如果ELBO收敛，则结束，否则返回第一步</li></ol><h2 id="五、业务CASE的平均场变分推断求解"><a href="#五、业务CASE的平均场变分推断求解" class="headerlink" title="五、业务CASE的平均场变分推断求解"></a>五、业务CASE的平均场变分推断求解</h2><p>为了求解第三节中的问题，根据平均场变分推断的思路，我们先假设隐变量$u_k$的分布满足均值为$m_k$, 方差为$s_k$，而隐变量$c_i$由参数$\varphi_i$决定, 参数也是k维向量，每一维度指定了$c_i$对应维度为1的概率，即:</p><script type="math/tex; mode=display">q(c_i;\varphi_i)=\sum_k c_{ik}\varphi_{ik} \tag{3}</script><p>于是根据平均场假设有:</p><script type="math/tex; mode=display">p(\mathbf \mu, \mathbf c)=\prod_{k}q(\mu_k;m_k,s_k)\prod_iq(c_i;\varphi_i)</script><p>式子(1)被重写为:</p><script type="math/tex; mode=display">p(\mathbf \mu, \mathbf c, \mathbf x) = \prod_k p(\mu_k;m_k,s_k) \prod_i p(c_i;\varphi_i) p(x_i|c_i,\mathbf \mu)</script><p>于是ELBO为:</p><script type="math/tex; mode=display">\begin {aligned}ELBO(\mathbf m, \mathbf s,\mathbf \varphi) &= \sum_k \mathbb E_{q(\mu_k;m_k,s_k)} \left[ \log p(u_k;m_k,s_k) \right] \\ &+ \sum_i \left( \mathbb E_{q(c_i;\varphi_i)} \left[ \log p(c_i; \varphi_i) \right] + \mathbb E_{q(c_i;\varphi_i)} \left[ \log p(x_i|c_i,\mathbf \mu; \mathbf m, \mathbf s, \varphi_i) \right]  \right) \\& - \sum_k \mathbb E_{q(\mu_k;m_k,s_k)}\left[ \log q(\mu_k;m_k, s_k) \right] \\& - \sum_i \mathbb E_{q(c_i;\varphi_i)} \left[ \log q(c_i; \varphi_i) \right]\end {aligned} \tag{4}</script><p>在算法运行的每一轮迭代中，先求解$q(c_i; \varphi_i)$：</p><script type="math/tex; mode=display">\begin {aligned}\log q(c_i; \varphi_i) &= \mathbb{E}_{q(\mathbf \mu;\mathbf m,\mathbf s)} \left[ \log p(\mathbf c,  \mathbf{\mu}, \mathbf{x}) \right] \\&= \log p(c_i) +  \mathbb{E}_{q(\mathbf \mu;\mathbf m,\mathbf s)} \left[ \log p(x_i|c_i,\mathbf \mu; \mathbf m, \mathbf s, \varphi_i) \right]\end {aligned}</script><p>根据业务上的定义$\log p(c_i)=-\log k$为常数，并且根据$c_i$的定义有$p(x_i|c_i, \mathbf \mu)=\prod_k p(x|\mu_k)^{c_{ik}}$, 其中$c_{ik}$为$c_i$的第k维值，于是</p><script type="math/tex; mode=display">\begin{aligned}\log q(c_i; \varphi_i) &= \mathbb{E}_{q(\mathbf \mu;\mathbf m,\mathbf s)} \left[ \log p(x_i|c_i,\mathbf \mu; \mathbf m, \mathbf s, \varphi_i) \right] + const\\&= \sum_k c_{ik}\mathbb{E}_{q(\mathbf \mu_k;m_k,s_k)} \left[ \log p(x_i|\mathbf \mu_k; m_k, s_k) \right] + const \\&= \sum_k c_{ik}\mathbb{E}_{q(\mathbf \mu_k;m_k,s_k)} \left[ \frac{-(x_i-\mu_k)^2}{2}; m_k, s_k \right] + const \\&= \sum_k c_{ik} \left( \mathbb{E}_{q(\mathbf \mu_k;m_k,s_k)} \left[ \mu_k;m_k, s_k \right]x_i - \frac{1}{2} \mathbb{E}_{q(\mathbf \mu_k;m_k,s_k)} \left[\mu_k^2;m_k,s_k\right] \right) + const\end{aligned} \tag{5}</script><p>对比式子(3)和(5), 很显然有:</p><script type="math/tex; mode=display">\varphi_{ik} = e^{\mathbb{E}_{q(\mathbf \mu_k;m_k,s_k)} \left[ \mu_k;m_k, s_k \right]x_i - \frac{1}{2} \mathbb{E}_{q(\mathbf \mu_k;m_k,s_k)} \left[\mu_k^2;m_k,s_k\right]}</script><p>在这里根据$q(\mu_k;m_k,s_k)$的定义，可以知道$\mathbb{E}_{q(\mathbf \mu_k;m_k,s_k)} \left[ \mu_k;m_k, s_k \right]x_i=m_kx_i$而$\mathbb{E}_{q(\mathbf \mu_k;m_k,s_k)} \left[\mu_k^2;m_k,s_k\right]=s_k^2+m_k^2$于是:</p><script type="math/tex; mode=display">\varphi_{ik}=m_k-\frac{1}{2}(s_k^2+m_k^2) \tag{6}</script><p>接下来 继续来求解$q(\mu_k;m_k,s_k)$</p><script type="math/tex; mode=display">\begin {aligned}\log q(\mu_k;m_k,s_k) &= \log p(\mu_k) + \sum_i \mathbb E_{q(c_i,\mathbf \mu_{k})}\left[  \log p(x_i|c_i, \mathbf \mu_k; \varphi_i, \mathbf m_{-k}, \mathbf s_{-k}) \right] \\&= -\frac{\mu_k^2}{2\sigma^2} + \sum_i \varphi_{ik} \left(\frac{-(x_i-u_k)^2}{2} \right) + const \\&= \mu_k \left( \sum_i\varphi_{ik}x_i \right) - \mu_k^2 \left( \frac{1}{2\sigma^2} + \sum_i \frac{\varphi_{ik}}{2} \right) + const\end {aligned} \tag{7}</script><p>观察高斯分布的形式$p(x)=\frac{1}{\sqrt{2\pi}s}e^{\frac{-(x-\mu)^2}{2s^2}}$于是有:</p><script type="math/tex; mode=display">\begin{aligned}\log p(x) &= -\log (\sqrt{2\pi}s) - \frac{(x-\mu)^2}{2s^2} \\&= -\log (\sqrt{2\pi}s) -  \frac{x^2}{2s^2}-\frac{\mu^2}{2s^2} + \frac{x\mu}{s^2}\end{aligned} \tag{8}</script><p>对比(7)和(8)式，可见实际上$q(u_k;m_k, s_k)$是一个高斯分布，于是可以令:</p><script type="math/tex; mode=display">\frac{1}{2s_k^2}=\frac{1}{2\sigma^2} + \sum_i \frac{\varphi_{ik}}{2}</script><p>可以解出来:</p><script type="math/tex; mode=display">s_k= \frac{1}{\sqrt{\frac{1}{\sigma^2}+\sum_i \varphi_{ik}}} \tag{9}</script><p>同理可以令:</p><script type="math/tex; mode=display">\frac{m_k}{s_k^2}=\sum_i \varphi_{ik}x_i \tag{10}</script><p>把（9）带入即可解得:</p><script type="math/tex; mode=display">m_k=\frac{\sum_i \varphi_{ik}x_i}{\frac{1}{\sigma^2}+\sum_i \varphi_{1k}} \tag{11}</script><p>最后我们得到了所有参数的更新公式，总结求解过程为:</p><ol><li>随机初始化所有参数$\mathbf \varphi,  \mathbf m, \mathbf s$</li><li>按照(6)式子更新每一个参数参数$\varphi_i$</li><li>按照(9)式更新每一个参数$s_k$</li><li>按照(10)式更新每一个参数$m_k$</li><li>按照(4)式计算ELBO,  如果ELBO收敛则结束，否则返回第1步</li></ol><h2 id="六、代码实现"><a href="#六、代码实现" class="headerlink" title="六、代码实现"></a>六、代码实现</h2><p>根据上述算法的描述，可以写出实现代码，但第5步可以不需要计算ELBO, 直接迭代n步之后结束即可，具体代码如下：</p><pre><code class="lang-python">def solve(x, k, sigma, ephco=20):    &quot;&quot;&quot;    x: 输入数据    k: 超参数k，c_i的维度，在业务CASE中等于用户数    sigma: 超参数，需要人工调整    &quot;&quot;&quot;    n = len(x)    phis = np.random.random([n, k])    mk = np.random.random([k])    sk = np.random.random([k])    for _ in range(epoch):        for i in range(n):            phi_i_k = []            for _k in range(k):                #根据公式(6)更新参数phi_ik                phi_i_k.append(np.exp(mk[_k]*x[i] - (sk[_k]**2 + mk[_k]**2)/2))            sum_phi = sum(phi_i_k)            phi_i_k = [phi/sum_phi for phi in phi_i_k]            phis[i] = phi_i_k        den = np.sum(phis, axis=0) + 1/(sigma**2)        #根据公式(10)更新m_k        mk = np.matmul(x, phis)/den        #根据公式(11)更新s_k        sk = np.sqrt(1/den)    return mk, sk, phis</code></pre><p>输入第二节中生成的数据x和超参数后，求解得到$m=[-32.44, -20.15, -8.14, -7.89, 2.79, 3.15, 13.78, 14.72, 15.65, 26.58]$, 对比第二节的真实参数$u$十分接近。从图像上， 根据求解出来的参数$\mathbf m, \mathbf s, \mathbf \varphi$模拟采样数据，得到的数据分布也与真实数据分布十分接近(蓝色为真实数据，橙色为模拟采样数据)</p><p><img src="1576417800040.png" alt="1576417800040"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&quot;一、贝叶斯推断的工作流&quot;&gt;&lt;a href=&quot;#一、贝叶斯推断的工作流&quot; class=&quot;headerlink&quot; title=&quot;一、贝叶斯推断的工作流&quot;&gt;&lt;/a&gt;一、贝叶斯推断的工作流&lt;/h2&gt;&lt;p&gt;在贝叶斯推断方法中，工作流可以总结为：&lt;/
      
    
    </summary>
    
      <category term="机器学习" scheme="https://embolismsoil.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://embolismsoil.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="https://embolismsoil.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>策略梯度与A2C算法</title>
    <link href="https://embolismsoil.github.io/2019/08/24/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E4%B8%8EA2C%E7%AE%97%E6%B3%95/"/>
    <id>https://embolismsoil.github.io/2019/08/24/策略梯度与A2C算法/</id>
    <published>2019-08-24T11:44:15.000Z</published>
    <updated>2019-12-15T03:12:04.515Z</updated>
    
    <content type="html"><![CDATA[<h2 id="从Q-learning到策略梯度"><a href="#从Q-learning到策略梯度" class="headerlink" title="从Q learning到策略梯度"></a>从Q learning到策略梯度</h2><p>在解决<code>MDP</code>问题的算法中，<code>Value Base</code>类算法的思路将关注点放在价值函数上，传统的<code>Q Learning</code>等算法是一个很好的例子。<code>Q Learning</code>通过与环境的交互，不断学习逼近<code>(状态, 行为)</code>价值函数$Q(s_t, a_t)$，而策略本身即选取使得在特定状态下价值函数最大的动作，即$a_t = \mathop{\arg\min}_{a}Q(s_t, a)$ ， 具体算法如图1所示。</p><p><img src="1566647970040.png" alt="Q Learning算法"></p><p>其中$Q(S, A) \leftarrow Q(S, A)+\alpha\left[R+\gamma \max _{a} Q\left(S^{\prime}, a\right)-Q(S, A)\right]$一步即时序差分法的价值函数逼近过程，具体原理详见。</p><p>Q learning算法已经能解决许多问题，但最致命的一点是: 在确定环境$s_t$下，策略选择的行动总是确定的，这对于很多场景来说，并不适用。例如玩剪刀石头布的时候，如果出拳的策略是一定的话，就很容易被对手察觉并击破。同时，Q learning也无法解决状态重名的问题。具体地说，状态重名是指在两个现实中的状态，在建模中表现出来的<code>state</code>是一样的，也就是$s_t$向量的每个维度都相等。如下图中格子世界的例子，如果状态被建模成二维向量，维度分别表示左右是否有墙阻挡，那么图中两个灰色格子的状态向量是一样的，于是他们在Q learning中学习到的策略会选择一样的行动，但矛盾的是: <strong>如果选择向左走，对于第一个格子就是一次失败的决策。如果选择向右走，对于第二个格子来说就是一次失败的决策</strong>。特别是如果使用$\epsilon-greedy$策略时，很可能在第一个灰格子会不停选择向左的行动，直到一次$\epsilon$概率的事件发生时，才有可能选择一次随机行为，从而有机会跳出这个坏处境。这时候还不如直接使用随机策略管用。</p><p><img src="1566648579995.png" alt="格子世界"></p><p>针对上述种种缺点，策略梯度法应运而生。</p><p>首先，我们需要明确的是，强化学习的最终目的是最大化价值函数。Q learning算法的思路比较绕，Q learning并没有直接去最大化价值函数，而是思考: 在给定状态$s_t$下，做出动作$a_t$会得到什么样的回报？ 得到答案之后，每次都贪婪地选择回报最大的那个动作。 可是为什么我们不直接思考: 在给定状态下，做出什么样的动作，才能让回报最大化？ 策略梯度就是这样一个直接的算法。</p><p>具体地说，策略梯度算法将策略建模成为$\pi_{\theta}(s,a)$，表示在$s$状态下选择$a$动作的概率，其中$\theta$为参数。并且将负回报函数作为损失函数，应用梯度下降法将期望奖励最大化。定义为</p><script type="math/tex; mode=display">J(\theta)=\sum_{s} d(s) \sum_{a} \pi_{\theta}(s, a) \mathcal{R}(s,a) \tag{1}</script><p>这样，(1)式对参数$\theta$求梯度得到</p><script type="math/tex; mode=display">\begin{aligned} \nabla_{\theta} J(\theta) &=\sum_{\mathbf{s} \in S} d(s) \sum_{a \in A} \pi_{\theta}(s, a) \nabla_{\theta} \log \pi_{\theta}(s, a) \mathcal{R}_{s, a} \\ &=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) \mathcal{R}(s,a)\right] \end{aligned}  \tag{2}</script><p>式子(2)的期望通过均值代替得到</p><script type="math/tex; mode=display">\nabla_{\theta}J(\theta)=\frac{1}{N}\sum{\nabla_{\theta}\log\pi_{\theta}(s, a)\mathcal{R}(s,a)}   \tag{3}</script><p>于是我们得到了蒙特卡洛策略梯度算法</p><p><img src="1566661134557.png" alt="蒙特卡洛策略梯度"></p><h2 id="AC算法"><a href="#AC算法" class="headerlink" title="AC算法"></a>AC算法</h2><p>从式子(3)来看蒙特卡洛策略梯度算法在策略梯度更新的过程中，考虑的是即时奖励$v_t$，而即时奖励具有较大噪声，为了得到更稳定的表现，可以使用长期回报来替代即时奖励。具体如式(4):</p><script type="math/tex; mode=display">\begin{aligned} \nabla_{\theta} J(\theta) &=\sum_{\mathbf{s} \in S} d(s) \sum_{a \in A} \pi_{\theta}(s, a) \nabla_{\theta} \log \pi_{\theta}(s, a) G_{s,a} \end{aligned}   \tag{4}</script><p>其中$G_{s,a}=\sum\lambda^n\mathcal{R}_{n}$定义为(s,a)的长期回报, 根据Q函数的定义$Q(s, a)=\mathbb{E}[G_{s,a}|s,a]$,于是式子(4)使用长期回报期望$Q(s, a)$直接替代长期回报得到式(5)</p><script type="math/tex; mode=display">\nabla_{\theta}J(\theta) = \mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) Q(s,a)\right] \tag{5}</script><p>于是根据(5)式我们可以得到$\Delta \theta=\nabla_{\theta}\log \pi_{\theta}(s, a) Q(s,a)$，用这种方式更新参数的就是Actor-Critic算法，简称AC算法。其中Critic就是$Q(s,a)$，本质上就是梯度权值，也可以说是评价梯度的重要性。假设我们使用的Q函数是一个简单的线性函数$Q_w(s,w)=\phi(s,a)^Tw$，那么AC算法具体的过程可以给出如下图。</p><p><img src="1566668496252.png" alt="1566668496252"></p><h2 id="A2C算法"><a href="#A2C算法" class="headerlink" title="A2C算法"></a>A2C算法</h2><p>AC算法使用的Q函数是一个随机初始化的函数，需要在交互中学习逼近真正的$\hat{Q}$，这意味着我们在梯度更新中引入了噪声，或者说方差。为了解决这个问题，A2C引入了Baseline的概念。具体地说是通过在(5)式中引入一个Baseline函数$\mathcal{B}$得到(6)式子</p><script type="math/tex; mode=display">\nabla_{\theta}J(\theta) = \mathbb{E}_{\pi_{\theta}}\left\{\nabla_{\theta} \log \pi_{\theta}(s, a)\left[ Q(s,a)-\mathcal{B}\right]\right\} \tag{6}</script><p>且要求(5)式与(6)相等(期望不变)但方差更低。事实上只要$\mathcal{B}$ 只与s相关而与a无关，即$\mathcal{B}(s)$就可以达到期望不变的目的。简单地将(6)式子展开即可得到这个结论</p><script type="math/tex; mode=display">\begin{aligned} \nabla_{\theta}J(\theta) & = \mathbb{E}_{\theta}\left[\nabla_{\theta}\log\pi_{\theta}Q(s,a)\right]-\mathbb{E}\left[\nabla_{\theta}\log\pi_{\theta}(s,a)\mathcal{B}(s)\right] \\ &=  \mathbb{E}_{\theta}\left[\nabla_{\theta}\log\pi_{\theta}Q(s,a)\right]-\sum_{s\in S}d^{\pi_{\theta}(s)} \mathcal{B}(s)  \nabla_{\theta} \sum_{a\in A}\log\pi_{\theta}(s,a)  \\ &= \mathbb{E}_{\theta}\left[\nabla_{\theta}\log\pi_{\theta}Q(s,a)\right]   \end{aligned}</script><p>第二个等号交换了求导与求和的顺序，并且将与a无关的$\mathcal B(s)$提到求和符号外，于是根据定义$\sum_{a \in A}\pi_{\theta}(s,a)=1$，而常数的梯度等于0。于是现在对于函数$\mathcal B(s)$只剩下<strong>让方差更低</strong>这一约束了。首先来看方差</p><script type="math/tex; mode=display">\begin{aligned} Var(X) = \mathbb E \left[ (X - \overline X)^2 \right] =\mathbb E (X^2)-[E(\overline X)]^2 \end{aligned}</script><p>接下来我们让方差对函数$\mathcal B(s)$的导数为0</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial Var(X)}{\partial \mathcal B (s)} &= \frac{\partial Var(X)}{\partial X} \cdot \frac{\partial X}{ \mathcal B(s) } \\ &= \mathbb 2E[X \cdot \frac{\partial X}{\partial \mathcal B(s)}] \\ &= 0\end{aligned}</script><p>然后带入$X=\nabla_{\theta} \log \pi_{\theta}(s, a)\left[ Q(s,a)-\mathcal{B}(s)\right]$得到</p><script type="math/tex; mode=display">\begin{aligned} & \sum_{s \in S}d^{\pi_{\theta}}(s) \sum_{a \in A} [\nabla_{\theta}\log\pi_{\theta}(s,a)]^2[Q(s,a)-\mathcal B(s)]  \\ &= \sum_{s \in S}d^{\pi_{\theta}}(s) \sum_{a \in A} [\nabla_{\theta}\log\pi_{\theta}(s,a)]^2Q(s,a) -   \sum_{s \in S}d^{\pi_{\theta}}(s) \mathcal B(s)  \sum_{a \in A} [\nabla_{\theta}\log\pi_{\theta}(s,a)]^2\\ &= 0 \end{aligned} \tag{7}</script><p>解得</p><script type="math/tex; mode=display">\mathcal B(s) = \frac{\sum_{a \in A} [\nabla_{\theta}\log\pi_{\theta}(s,a)]^2Q(s,a)}{\sum_{a \in A} [\nabla_{\theta}\log\pi_{\theta}(s,a)]^2}  \tag{8}</script><p>式(7)给出了使得方差最小时得$\mathcal B(s)$，但也可以看到其计算复杂度十分高。事实上我们可以在计算复杂度和噪声指标上做权衡。从式子(7)中其实我们可以看到只要$\mathcal B (s)$逼近$Q(s,a)$且与a无关，即可得到一个接近最优解得方案。可以非常直觉地想到取状态价值函数$V(s)=\mathbb E[G_{s,a}|s]$作为$\mathcal B(s)$，即</p><script type="math/tex; mode=display">\mathcal B(s)=V(s) \tag{9}</script><p>最后，令$A(s,a)=Q(s,a)-V(s)$为优势函数(动作a相对平均表现的优势)，可以得到A2C算法的梯度公式</p><script type="math/tex; mode=display">\nabla_{\theta}J(\theta) = \mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a)A(s,a)\right] \tag{10}</script><p>在工程实现上，我们并不需要维持两套参数去分别交互逼近$Q(s,a)$和V(s)。具体地说，我们可以使用$\delta^A=r+\lambda V(s’)-V(s)$来替代$\delta =r+\lambda Q(s’,a’)-Q(s,a)$，因为根据定义$\mathbb E(\delta)=\delta^A$。并且恰好$\delta^A$就是$A(s,a)$的无偏估计，这是因为根据Q函数的定义有$ \mathbb E[r+\lambda V(s’)|s,a] = Q(s,a)$。所以实际上实现A2C算法的时候，只需要维持一套参数用于估计$V(s)$，并且做梯度下降更新参数的时候可以使用</p><script type="math/tex; mode=display">\Delta \theta=\alpha\nabla_{\theta}\log\pi_{\theta}(s,a)(r+\lambda V(s')-V(s))  \tag{11}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;从Q-learning到策略梯度&quot;&gt;&lt;a href=&quot;#从Q-learning到策略梯度&quot; class=&quot;headerlink&quot; title=&quot;从Q learning到策略梯度&quot;&gt;&lt;/a&gt;从Q learning到策略梯度&lt;/h2&gt;&lt;p&gt;在解决&lt;code&gt;MDP&lt;/
      
    
    </summary>
    
      <category term="强化学习" scheme="https://embolismsoil.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://embolismsoil.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="https://embolismsoil.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="计算广告" scheme="https://embolismsoil.github.io/tags/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"/>
    
  </entry>
  
  <entry>
    <title>拍卖与博弈：计算广告中的底价问题</title>
    <link href="https://embolismsoil.github.io/2019/07/21/%E6%8B%8D%E5%8D%96%E4%B8%8E%E5%8D%9A%E5%BC%88-%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E4%B8%AD%E7%9A%84%E5%BA%95%E4%BB%B7%E9%97%AE%E9%A2%98/"/>
    <id>https://embolismsoil.github.io/2019/07/21/拍卖与博弈-计算广告中的底价问题/</id>
    <published>2019-07-21T05:19:46.000Z</published>
    <updated>2019-08-24T11:42:10.027Z</updated>
    
    <content type="html"><![CDATA[<h2 id="流量交易"><a href="#流量交易" class="headerlink" title="流量交易"></a>流量交易</h2><p>现代计算广告中，最广泛的流量交易模式为实时竞价模式，即Real-Time-Bidding(RTB)。实时竞价顾名思义，就是在流量到达时被放到交易市场进行公开的，实时的竞拍，参与竞拍的广告主赢得竞拍后，即可获得对这个流量的投放权，整个流程如图示。</p><p><img src="https://blog.fyber.com/wp-content/uploads/2015/02/RTB_Infographic.png" alt="RTB系统示意图"></p><p>以新浪微博的信息流广告为例，当我们刷微博时，微博的信息流(或者timeline)中会夹杂着广告，假设微博将信息流的第4和第10位作为广告位，那么每次你刷新微博后，list中的第4和第10条微博总会是推广(广告)微博。于是当用户A进入微博的信息流界面时，除了从服务器请求微博内容之外，还会同时向RTB请求广告内容，这个请求称之为曝光请求。RTB接受到该曝光请求后，将此次曝光放入流量市场进行实时拍卖，诸多广告主的投放系统(DSP)接到拍卖的事件后，立即对该曝光请求进行出价，一般来说根据曝光质量和属性不同，各个广告主对其出价也不尽相同。例如信息流第4位的曝光出价就比信息流第10位出价高，因为排在前面的广告更能引起用户注意，再如篮球鞋销售商对男性微博用户的曝光请求出价要比女性微博用户的曝光出价要高，因为篮球鞋的受众群体主要是男性。在广告主出价完成后，RTB则向出价最高的广告主请求广告内容，并且将此内容返回到微博客户端，于是用户在刷微博时就看到了这个广告，完成了一次广告曝光。这整个请求、竞价、返回广告的过程会在毫秒级别内完成。</p><h2 id="第二高价和底价问题"><a href="#第二高价和底价问题" class="headerlink" title="第二高价和底价问题"></a>第二高价和底价问题</h2><p>在RTB的流量竞拍中是按照<strong>第二高价</strong>来计费的，这意味着如果第$t$次曝光n个广告主对其出价分别为$b_1(t),b_2(t),b_3(t),…,b_n(t)$，且有$b_1(t)&gt;b_2(t)&gt;b_3(t)&gt;…&gt;b_n(t)$，那么广告主1竞价胜出，但是媒体方只会向广告主1收费$b_2$元。这种计费方式主要可以让广告主失去调价的动力，否则在广告主竞价胜出之后，会立即调低出价，以期望使用更低的价格拿到曝光，从而提高自身收益。但是从媒体方的角度来说，第二高价的机制存在着$\delta = b_1(t)-b_2(t)$的利润空间没有充分利用，于是引出了底价机制。底价机制指的是RTB在竞价拍卖的过程中，设置一个最低价格$\alpha$, 称之为底价。当$b_1(t)&lt;\alpha$时则RTB则拒绝售卖该流量，或者将流量导入其它交易市场售卖。当$b_1(t) \ge \alpha&gt;b_2(t)$时，按照$\alpha$向胜出的广告主收费，当$b_2(t) \ge \alpha$时，则按照$b_2(t)$向胜出的广告主收费。总结起来，在第$t$次竞价中，向竞价胜出的广告主收费$r(t)$如式(1)所示。</p><script type="math/tex; mode=display">r(t)=\left\{\begin{array}{ll}{\alpha,} & {b_{1}(t) \geq \alpha>b_{2}(t)} \\ {b_{2}(t),} & {b_{2}(t) \geq \alpha} \\ {0,} & {\alpha>b_{1}(t)}\end{array}\right. \tag{1}</script><p>从$r(t)$的表达式中可以看到，当满足$b_1(t) \ge \alpha &gt; b_2(t)$时，媒体放可以比原来的纯二价计费方式多赚$\beta(t)=\alpha-b_2(t)$，但也要注意到当$\alpha &gt; b_1(t)$时也存在损失$b_2(t)$的风险。于是一个很自然的想法就是：</p><blockquote><p>让$\alpha$能自动调整，让$\alpha$在$b_1(t)$较大时，$\alpha$也能随之增大，反之则减小。 — 原则[1]</p></blockquote><p>这就是动态底价的概念，我们重新把这个可以变化的底价定义为$\alpha(t)$，表示在第$t$次竞价中的底价。式子(1)重新表述为式(2)。</p><script type="math/tex; mode=display">r^{\prime}(t)=\left\{\begin{array}{ll}{\alpha(t),} & {b_{1}(t) \geq \alpha(t)>b_{2}(t)} \\ {b_{2}(t),} & {b_{2}(t) \geq \alpha(t)} \\ {0,} & {\alpha(t)>b_{1}(t)}\end{array}\right. \tag{2}</script><p>目前动态底价的算法主要有三种:</p><ol><li>基于贝叶斯推断的底价估计算法</li><li>基于均值统计的底价估计算法</li><li>基于经验的One-Shot底价调整算法</li></ol><p>下面逐一介绍这三个算法。</p><h2 id="基于贝叶斯推断的底价估计算法"><a href="#基于贝叶斯推断的底价估计算法" class="headerlink" title="基于贝叶斯推断的底价估计算法"></a>基于贝叶斯推断的底价估计算法</h2><p>该算法首先假设在一次RTB竞价的过程中，最高出价$B_1$为满足对数高斯分布的随机变量，并且假设其方差已知(可以当成超参数来调)，而其均值的先验分布为高斯分布，即:</p><script type="math/tex; mode=display">\begin{aligned} B_{1} & \sim \operatorname{lognorm}\left(\mu, \sigma^{2}\right) \\ \mu(t) & \sim \mathcal{N}\left(\theta(t), \delta^{2}(t)\right) \end{aligned} \tag{3}</script><p>假设$\mu(t)$的分布在系统上线前已经由历史时刻$[0,t]$的数据拟合得到了$\theta(t)$和$\delta^2(t)$，当系统在线运行时，$t+1$竞拍结束后得到一个最高出价的样本$b_1(t+1)$，这样由于对数高斯分布和高斯分布为共轭分布，可以直接后验分布</p><script type="math/tex; mode=display">\begin{aligned}    p(\theta(t) | b_1(t+1))=\mathcal{N}\left(\theta(t+1) | \theta(t), \delta^{2}(t)\right)\end{aligned} \tag{4}</script><p>其中</p><script type="math/tex; mode=display">\begin{aligned} \theta(t+1) &=\frac{\theta(t) \delta^{2}(t)+\sigma^{2} b_{1}(t+1)}{\sigma^{2}+\delta^{2}(t)} \\ \delta^{2}(t+1) &=\frac{\delta^{2}(t) \sigma^{2}}{\sigma^{2}+\delta^{2}(t)} \end{aligned} \tag{5}</script><p>这样根据最大后验估计的原则，我们可以使用$\theta(t)$来作为$\mu$的估计，并且根据式(3)可知，可以将$log(\theta(t))$作为对$B_1$的估计。根据原则[1], 只要简单地设置:</p><script type="math/tex; mode=display">\alpha(t+1) = log(\theta(t+1)) \tag{6}</script><p>可以看到基于贝叶斯推断的方法首先估计了$b_1(t+1)$，然后通过设置$\alpha(t+1)$逼近$b_1(t+1)$的方式来最大化本次曝光的收益。</p><h2 id="基于均值统计的底价估计算法"><a href="#基于均值统计的底价估计算法" class="headerlink" title="基于均值统计的底价估计算法"></a>基于均值统计的底价估计算法</h2><p>均值估计的想法其实十分朴素：<strong>本次曝光的收益应当不低于历史平均收益</strong>。于是很直接的，使用$t-1$时刻的历史平均收益来作为$\alpha(t)$的估计：</p><script type="math/tex; mode=display">\alpha(t)=\frac{1}{M} \sum_{i=t-M}^{t-1} r(i) \tag{7}</script><p>当然，线上系统的运行环境是会发生改变的，比如说流量分布，竞争环境，等等。所以收益分布应当也不是一个静态的分布，并且当样本越靠近当前时刻，则越有可能采样于当前的收益分布，所以计算平均收益时，应当使用加权平均的方式来进行，并且设置离当前时刻越近的数据，权重就越高:</p><script type="math/tex; mode=display">\alpha(t)=\frac{1}{M} \sum_{i=t-M}^{t-1} w(i, t) r(i) \tag{8}</script><p>其中$w(i,t)$的选择可以多种多样，只要满足<strong>当前时刻越近的数据，权重就越高</strong>的原则即可。</p><h2 id="ONE-SHOT底价调整算法"><a href="#ONE-SHOT底价调整算法" class="headerlink" title="ONE-SHOT底价调整算法"></a>ONE-SHOT底价调整算法</h2><p>ONE-SHOT其实是一个调整算法而不是一个估计算法。其工作的流程是:</p><ol><li>给定一个初始底价$\alpha(t)$</li><li>根据一定规则在$\alpha(t)$的基础上调整得到$\alpha(t+1)$</li></ol><p>初始底价可以根据经验直接指定，也可以通过式子(7)对历史数据统计给出。而调整的原则在于:</p><blockquote><p>当底价低于$b_1$时，则应当缓慢提高底价，当底价高于$b_1$时应当迅速降低底价  —原则[2]</p></blockquote><p>原则[2]提出的基本思想是：当底价低于$b_1$时，虽然此时收益并非是最大化的，但毕竟有$b_2$作为保证，收益不会太低，所以底价上涨探索最优的收费不必太过于急切，但如果底价高于$b1$时，收益将会直接变为0，所以需要快速降低底价，让$b_2$来保证收益。</p><p>根据原则[2]，当然可以设置固定的小步伐$s_u$和大步伐$s_l$来调整底价。但是为了平滑和鲁棒性，可以根据式(9)来执行调整过程。</p><script type="math/tex; mode=display">\left\{\begin{array}{ll}{\alpha(t+1)=\left(1-\epsilon^{t} \lambda_{h}\right) a(t)} & {\text { if } \alpha(t)>b_{1}(t)} \\ {\alpha(t+1)=\left(1+\epsilon^{t} \lambda_{e}\right) a(t)} & {\text { if } b_{1}(t) \geq \alpha(t) \geq b_{2}(t)} \\ {\alpha(t+1)=\left(1+\epsilon^{t} \lambda_{l}\right) \alpha(t)} & {\text { if } b_{2}(t)>\alpha(t)}\end{array}\right. \tag{9}</script><p>其中$\epsilon \in (0, 1]$且$\lambda_h, \lambda_e, \lambda_l \in [0, 1]$, $\epsilon$是一个时间衰减系数，如果RTB只希望在启动后个某个时间段调整底价，并且最终收敛，则可以通过这个参数来调整。而$\lambda_h$则用于调整当$\alpha(t) &gt; b_1(t)$时底价的降低速度。而同理$\lambda_l$用来调整当$\alpha(t) &lt; b_2(t)$时底价的提升速度。$\lambda_e$则用于调整探索最优收益的速度。从业务上来说，往往会设置$\lambda_h&gt;\lambda_l&gt;\lambda_e$,即当前收益越高，调整速度越慢，这与原则[2]的基本思想是一致的。</p><h2 id="其他讨论"><a href="#其他讨论" class="headerlink" title="其他讨论"></a>其他讨论</h2><p>上述三种底价算法里，基于贝叶斯推断的算法的数学理论是最完备的，但必须要考虑到这种算法存在<strong>最高出价符合对数高斯分布</strong>这个假设前提的。这个假设并非放之四海而皆准的，各个广告系统的业务不一样，符合的分布也不一样，需要选择合适的分布，才能做出好的效果。并且需要注意的是，该算法优化的是单次曝光收益，并没有从全局的视角来考虑底价问题，不一定在整体收益上是最高的。</p><p>基于统计均值的算法是最朴素的，也是最容易实现的，不失为一个短平快的解决方案，它应当作为任何动态底价第一版本的最好选择，至少你能有一个Baseline去观察你的系统，判断动态底价对于你的广告系统来说是否是必要的，是否真的能提升收入。</p><p>而ONE-SHOT算法虽然没有数学理论支撑，但它作为一种博弈思维的产物，有从业务和全局的视角去考虑底价问题，从直觉上来看是最值得尝试的一种。当然，我想ONE-SHOT算法应当还有许多改进的空间，比如$\lambda_h, \lambda_e,\lambda_l$参数的值可以跟贝叶斯推断来动态调整的值，也可以通过反馈控制的方法来动态调整。</p><p>但最终，无论如何，这里都没办法给哪一种算法的优劣性下一个断论，毕竟业务多种多样，只能认真做好A/B Test，好好调参，根据实际业务表现来选择算法了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;流量交易&quot;&gt;&lt;a href=&quot;#流量交易&quot; class=&quot;headerlink&quot; title=&quot;流量交易&quot;&gt;&lt;/a&gt;流量交易&lt;/h2&gt;&lt;p&gt;现代计算广告中，最广泛的流量交易模式为实时竞价模式，即Real-Time-Bidding(RTB)。实时竞价顾名思义，就是在
      
    
    </summary>
    
      <category term="计算广告" scheme="https://embolismsoil.github.io/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"/>
    
    
      <category term="机器学习" scheme="https://embolismsoil.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="https://embolismsoil.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="计算广告" scheme="https://embolismsoil.github.io/tags/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"/>
    
  </entry>
  
  <entry>
    <title>让机器读懂文章-pLSA模型推导及实现</title>
    <link href="https://embolismsoil.github.io/2019/07/12/%E8%AE%A9%E6%9C%BA%E5%99%A8%E8%AF%BB%E6%87%82%E6%96%87%E7%AB%A0-pLSA%E6%A8%A1%E5%9E%8B%E6%8E%A8%E5%AF%BC%E5%8F%8A%E5%AE%9E%E7%8E%B0/"/>
    <id>https://embolismsoil.github.io/2019/07/12/让机器读懂文章-pLSA模型推导及实现/</id>
    <published>2019-07-11T18:09:57.000Z</published>
    <updated>2019-07-12T18:38:01.140Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>人类读懂文章是一个很自然的行为，当我们读完一篇《背影》的时候，我们就可以知道这篇文章在写些什么，也就是我们说获得了这篇文章的相关知识。有了这些知识，我们就可以回答一些问题，例如:</p><ol><li>问：这篇文章写的主要内容是什么呢？<br> 答： 亲情、送别</li><li>问：有类似《背影》这样的文章可以推荐的吗？<br> 答：龙应台-《送别》</li></ol><p>虽然上面的问答对任务对人类来说十分简单，但对于机器来说却并不容易。机器对自然语言(中文文本)的理解实际上并不是非常简单的事情，因为自然语言本身是一个高层抽象的概念，而机器只擅长处理量化的知识，例如说让机器记住向量$\vec x=[1, 2, 3]$和$\vec y=[4, 5, 6]$是十分容易的事情，而且可以轻易知道$\vec x$和$\vec y$的相似程度，这只需要计算其记录即可，于是我们对于向量来说就可以完成上面的问题2了。</p><p>让我们重新揣摩一下人类读懂文章的过程，实际上我们并不需要背熟每一个字词，而是阅读完成之后再总结出这篇文章主要在写什么，也就是文章的主题。为了让机器能理解文章，我们也需要把这些主题量化出来，形成类似$\overrightarrow {topic}=[‘亲情’: 0.5, ‘送别’: 0.5]$的向量，这种能量化文章主题的模型，也就叫做<strong>主题模型</strong>了。</p><p>在主题模型方面前人们已经做了很多工作，并且取得了非常不错的成效，其中影响较大的是一类模型叫做<strong>隐语义模型</strong>，而这类模型里面<strong>概率隐语义分析</strong>也就是本文所述的pLSA则是应用最成功的模型之一，同样成功的模型还有<strong>隐含狄利克雷分布</strong>，也就是大名鼎鼎的LDA主题模型，不过LDA与pLSA的思想一脉相承，只不过做了贝叶斯改造而已。</p><h2 id="pLSA模型"><a href="#pLSA模型" class="headerlink" title="pLSA模型"></a>pLSA模型</h2><p>事实上pLSA是在对我们写一篇文章的行为建模，我们先揣摩朱自清先生写《背影》的行为。首先我朱先生敲定了今天要写一篇《背影》，然后他开始构思了这篇文章的主题为：亲情、送别，并且朱先生认为这两部分的内容都几乎同等重要，也就是: $[‘亲情’: 0.5, ‘送别’: 0.5]$，朱先生开始动笔，于是当朱先生写下</p><blockquote><p>我买几个橘子去，你就在此地，不要走动。</p></blockquote><p>实际上是朱先生先前构思好的亲情、父子、送别这三个中心思想在影响着朱先生写下了这段话。于是在这三个中心思想的影响下，朱先生写完了《背影》里面的所有词，而我们读者所谓的<strong>理解</strong>《背影》，实际上就是从我们看到的《背影》的所有词，推断出了朱先生构思的主题: $[‘亲情’: 0.5, ‘送别’: 0.5]$。而pLSA则只是用数学化的形式描述这个过程, 这样一个形式化的过程在pLSA的眼里是这样的：</p><ol><li>从分布$p(d_m)$上采样选择了一篇文章$d_m$</li><li>对于文章$d_m$每一个词，从分布$p(z_k|d_m)$上采样一个生成一个主题$z_k$</li><li>从分布$p(w_n|z_k)$上采样生成了一个词$w_n$</li></ol><p>这个模型可以用plate notation更加简洁地描述：</p><p><img src="plsa.png" alt="pLSA模型"></p><p>图中的阴影部分的变量$d$和$w$对应着文章和文章的所有词，表示可观测的变量，$z$是观测不到的主题，我们称之为隐变量，两个框框左下角的$N$和$M$则分别表示$N$和$M$词独立重复试验。这个图所表达的文章生成过程跟上面的文字表述是一致的。</p><p>这样写文章的模型是符合直觉的，但仔细推敲总觉得过于机械生硬，这样的机械式过程能写得出朱先生《背影》那样优秀的文章吗? </p><blockquote><p>如果无限多个猴子任意敲打打字机键，最终会写出大英博物馆的所有藏书 — 爱丁顿无限猴子理论</p></blockquote><p>一件小概率的事件在多次试验中必然发生，这就是为什么随机敲打键盘的猴子也能作的原因，于是上面问题答案自然是肯定的，pLSA这样合乎直觉的模型当然要比一只茫无目的敲打键盘的猴子更加具备写作天赋。</p><p>我们读者需要阅读根据文章和文章的所有内容去推断文章的主题，而pLSA眼里则是根据可观测变量$w$和可观测变量$d$去推断隐变量$z$。我们可以通过海量的文章去解算出模型中的参数，也就是上文中的$p(z_k|p_m)$和$p(w_n|z_k)$两个分布，我们称之为<strong>文章主题分布</strong>和<strong>主题词分布</strong>。 而$p(z_k|d_m)$这个分布实际上就是文章$d_m$的主题分布，也就是我们前文所说的$[‘亲情’: 0.5, ‘送别’: 0.5]$这样的文章主题，这个分布就是我们就获取到关于文章的知识，它量化说明了文章$d_m$在说什么内容。至于模型参数解算的过程，这没什么不可以理解的，正如我定义了一个$y$的产生过程过$y=ax+b$, 当我拿到足够多的样本$y_0=0, y_1=1, y_2=2,….y_n=n$之后，实际上我可以将他们组成方程组解出合理的参数$a$、$b$和$x$来。</p><p>行文至此，我们且对pLSA的求解按下不表，先来实际感受一下pLSA的作用。这里选择格林童话中的十几篇童话作为语料训练pLSA，然后分别从5个主题分布中取出的top3词语：</p><div class="table-container"><table><thead><tr><th>topic-1</th><th>topic-2</th><th>topic-3</th><th>topic-4</th><th>topic-5</th></tr></thead><tbody><tr><td>wrong</td><td>birds</td><td>morning</td><td>soldier</td><td>good</td></tr><tr><td>issue</td><td>fox</td><td>met</td><td>king</td><td>gave</td></tr><tr><td>faith</td><td>horse</td><td>wood</td><td>castle</td><td>great</td></tr></tbody></table></div><p>可以看到pLSA是可以正确推导出来主题分布的。</p><h2 id="pLSA的EM算法推导"><a href="#pLSA的EM算法推导" class="headerlink" title="pLSA的EM算法推导"></a>pLSA的EM算法推导</h2><p>pLSA是一种含隐变量的生成模型，也就是概率化地描述了样本数据(文章)的生成并且包含隐藏变量的模型，对于这种模型可用MCMC或EM算法来求解。本文讲解的是pLSA的EM算法求解，这里并不打算讲解EM的具体推导，而是直接利用EM算法的结论来对pLSA模型求解，关于EM算法的内容读者可以自己网上搜罗一下资料，或者待我抽空再写一篇关于EM算法的文章。<br>在开始推导之前，我们先假设词库大小为$j$, 每篇文章都由词库中的词$w_j$构成。然后定义模型参数:</p><script type="math/tex; mode=display">\begin{aligned}\theta_{mk}=p(z_k|d_m) \\\psi_{kj} = p(w_j|z_k)\end{aligned} \tag{1}</script><p>根据EM算法的求解步骤，我们先根据plate notation写出联合分布:</p><script type="math/tex; mode=display">    p(\bf w, \bf z, \bf d) = \prod_m p(d_m) \prod_n p(w_{mn}|z_{mn})p(z_{mn}|d_m) \tag{2}</script><p>其中$d_m$表示第$m$篇文章， $w_{mn}$表示第$m$篇文章中的第$n$个词，$z_{mn}$表示第$m$篇文章中第$n$个词对应的主题。然后我们令给定模型参数下的主题后验证分布为：</p><script type="math/tex; mode=display">    Q(\bf z; \bf \theta, \bf \psi) = p(\bf z|\bf d, \bf w; \bf \theta, \bf \psi) \tag{3}</script><p>于是可以启动EM算法当中的求期望步骤：</p><script type="math/tex; mode=display">\sum_{\bf z} {Q(\bf z)lnp(\bf w, \bf z, \bf w)} = \sum_mlnp(d_m) \sum_n \sum_kq(z_{mnk})ln[p(w_{mn}|z_k)p(z_k|d_m)] \tag{4}</script><p>其中$q(z_{mnk})$表示在给定参数下的主题验分布，这里有:</p><script type="math/tex; mode=display">q(z_{mnk}) = p(z_k|d_m, w_n; \theta_{mk}, \psi_{kj}) = \frac {p(d_m)\theta_{mk}\psi_{kn}}{\sum_kp(d_m)\theta_{mk}\psi_{kn}} \tag{5}</script><p>由于文章中总会出现许多重复词，例如文章$d_m$中第1个词和第$5$个词是一样的，那么就会有$w_{m1}=w_{m5}=w_j$那么对于式子$(4)$中$\sum_n \sum_kq(z_{mnk})ln[p(w_{mn}|z_k)p(z_k|d_m)]$这部分，我们可以将文章$d_m$中重复出现的词对应的项合并成为$\sum_j n_{mj}\sum_kq(z_{mjk})ln[p(w_j|z_k)p(z_k|d_m)]$, 其中$n_{mj}$为文章$d_m$中词$w_j$出现的次数。于是我们重写式子$(4)$为：</p><script type="math/tex; mode=display">\sum_{\bf z} {Q(\bf z)lnp(\bf w, \bf z, \bf w)} = \sum_mlnp(d_m) \sum_j n_{mj}\sum_kq(z_{mjk})ln(\theta_{mk}\psi_{kj}) \tag{6}</script><p>我们的目标是最大化式子$(6)$, 并且因为参数$\bf \theta$和$\bf \psi$是概率分布，所以有要约束$\sum_k\theta_{km}=1$和$\sum_j{\psi_{kj}} = 1$, 并且由于$p(d_m)$这个先验证分布可以设置为常数，这样我们去除与优化无关的常数项和增加了约束之后，就可以得到整个带约束的优化目标:</p><script type="math/tex; mode=display">\begin{aligned}    \max \limits_{\theta_{mk}, \psi_{kj}}   \quad & \sum_m \sum_j n_{mj}\sum_kq(z_{mjk})ln(\theta_{mk}\psi_{kj}) \\    \bf{s.t.} \quad & \sum_{k}\theta_{mk}=1, m=1,2,3,...,M \\    & \sum_{j} \psi_{kj} = 1, k=1,2,3,...,K\end{aligned} \tag{7}</script><p>这个带约束的优化目标直接使用拉格朗日乘子法：</p><script type="math/tex; mode=display">    L(\bf \theta, \bf \psi, \bf \lambda, \bf \alpha) = \sum_m \sum_j n_{mj} \sum_k  q(z_{mjk})ln(\theta_{mk}\psi_{kj}) + \sum_m {\lambda_m} (1-\sum_k\theta_{mk}) + \sum_k \alpha_{k} (1-\sum_j {\psi_{kj}}) \tag{8}</script><p>于是可以对参数$\theta_{mk}$求导并令其为0:</p><script type="math/tex; mode=display">\frac{ \partial L(\bf \theta, \bf \psi, \bf \lambda, \bf \alpha)}{\partial \theta_{mk}} = \frac{ \sum_jn_{mj}q(z_{mjk})}{\theta_{mk}} - \lambda_m = 0 \\ \lambda_m \theta_{mk} ={ \sum_jn_{mj}q(z_{mjk})}  \tag{9}</script><p>式子(9)左右两边对$k$求和得到:</p><script type="math/tex; mode=display">\lambda_m \sum_k \theta_{mk} = \sum_j{n_{mj}} \sum_{z}q(z_{mjk}) \\\lambda_m = \sum_j {n_{mj}} = N_m \tag{10}</script><p>上述式子(10)中$N_m$表示文章$d_m$的总词数，将式子$(10)$代回式(9)可以得到:</p><script type="math/tex; mode=display">\theta_{mk} = \frac {\sum_j n_{mj}q(z_{mjk})}{N_m} \tag{11}</script><p>同样地我们对参数$\psi_{kj}$故技重施:</p><script type="math/tex; mode=display">\frac {\partial L(\bf \theta, \bf \psi,\bf \lambda, \bf \alpha)}{\partial \psi_{kj}} = \frac{\sum_m n_{mj}q(z_{mjk})}{\psi_{kj}} - \alpha_k = 0 \\\alpha_k \psi_{kj} = \sum_m n_{mj}q(z_{mjk}) \tag{12}</script><p>式子(12)左右两边对$j$求和得到:</p><script type="math/tex; mode=display">\alpha_k \sum_{j} \psi_{kj} = \sum_m \sum_j n_{mj} q(z_{mjk}) \\\alpha_k = \sum_m \sum_j n_{mj} q(z_{mjk}) \tag{13}</script><p>将式子代回$(12)$得到：</p><script type="math/tex; mode=display">\psi_{kj} = \frac { \sum_m n_{mj}q(z_{mjk})}{ \sum_m \sum_j n_{mj} q(z_{mjk})} \tag{14}</script><p>至此，pLSA参数求解完毕。根据参数更新的规则，我们设在EM算法迭代运行的过程中，第$i$轮的参数为$\theta_{mk}^i$和$\psi_{kj}^i$。于是整个pLSA的EM算法可以归纳为：</p><ol><li>随机初始化参数$\theta_{mk}^0$和$\psi_{kj}^0$</li><li>开始第$i\in[1, 2, 3…n]$轮迭代:<br> a. 求$q(z_{mjk})=\frac {p(d_m)\theta_{mk}^{i-1}\psi_{kj}^{i-1}}{\sum_kp(d_m)\theta_{mk}^{i-1}\psi_{kj}^{i-1}}$<br> b. 更新参数<script type="math/tex; mode=display">     \theta_{mk}^i = \frac {\sum_j n_{mj}q(z_{mjk})}{N_m} \\     \psi_{kj}^i = \frac { \sum_m n_{mj}q(z_{mjk})}{ \sum_m \sum_j n_{mj} q(z_{mjk})}</script> c. 若参数收敛，则退出迭代，否则返回<code>a</code>继续迭代</li><li>输出模型参数$\bf \theta$和$\bf \psi$</li></ol><h2 id="pLSA的实现"><a href="#pLSA的实现" class="headerlink" title="pLSA的实现"></a>pLSA的实现</h2><p>从上边的式子来看pLSA是相对比较容易实现的，但是高效地实现还需要一些技巧。首先看式(14)的分母，存在一个二阶求和的过程，如果语料库中有<code>1000</code>篇文档，<code>10000</code>个词，那么就要进行一千万次运算，这样显然必须要用并行批量计算的方式来加速，在实现上我们会将涉及的所有运算都转换为矩阵运算，这样就可以通过成熟的GPU库来加速运算。其次再看内存消耗问题，$q(z_{mjk})$总共需要储存<code>m*j*k</code>个参数，如果有<code>1000</code>篇文档<code>10000</code>个词和<code>50</code>个主题，那么$q(z_{mjk})$将有<code>5亿</code>个元素，这在内存消耗上是不可接受的，在实现上我们只会在批量计算$\theta_{mk}$和$\psi_{kj}$参数时用到的部分$q(z_{mjk})$批量计算出来，并且一旦使用完毕立即丢弃。具体代码就不在这里贴了，完整的demo见<a href="https://github.com/EmbolismSoil/pLSA" target="_blank" rel="noopener">pLSA实现</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>pLSA是概率隐语义主题模型中相对简单的一种，推导和实现都相对简单，回头看上面的算法过程，实际上只需要简单地计数迭代而已，所以pLSA非常适合在线学习。其实并非pLSA有此特点，事实上大多数生成模型都一样适合在线学习。不过pLSA的缺点也是非常明显的，<strong>pLSA将文章建模时没有考虑文章词序</strong>，也就是我们随机将一篇文章词打散，对于pLSA来说，其联合概率$p(\bf w, \bf z, \bf d)$是不变的,这一点回头看式子$(2)$就知道。这意味着”谁是你爸爸”和”你爸爸是谁”这两句话在pLSA眼里看来是一样的，这种情况在短文本场景中尤其常见。但幸运的是，在长文本领域，<em>有研表究明，汉字的序顺并不能影阅响读</em>。不过pLSA近年来正在逐渐被更新颖复杂的LDA代替，但相对LDA来说pLSA结构简单，容易做大规模并行化，所以时至今日，pLSA在大规模文本挖掘领域依旧光耀夺目。</p><p>最后，向Thomas Hofmann先生致敬，感谢先生为我们带来如此精妙的pLSA主题模型。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] <a href="https://arxiv.org/pdf/1301.6705.pdf" target="_blank" rel="noopener"> Probabilistic Latent Semantic Analysis</a><br>[2] <a href="https://arxiv.org/pdf/1212.3900.pdf" target="_blank" rel="noopener">Tutorial on Probablistic Latent Semantic Analysis</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;人类读懂文章是一个很自然的行为，当我们读完一篇《背影》的时候，我们就可以知道这篇文章在写些什么，也就是我们说获得了这篇文章的相关知识。有了这
      
    
    </summary>
    
      <category term="自然语言处理" scheme="https://embolismsoil.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="机器学习" scheme="https://embolismsoil.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="https://embolismsoil.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="自然语言处理" scheme="https://embolismsoil.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>svm优化问题的导出</title>
    <link href="https://embolismsoil.github.io/2019/07/12/svm%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E5%AF%BC%E5%87%BA/"/>
    <id>https://embolismsoil.github.io/2019/07/12/svm优化问题的导出/</id>
    <published>2019-07-11T17:53:57.000Z</published>
    <updated>2019-07-12T18:42:49.577Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SVM-优化问题的导出"><a href="#SVM-优化问题的导出" class="headerlink" title="SVM - 优化问题的导出"></a>SVM - 优化问题的导出</h1><p>svm的想法其实非常朴素：</p><ul><li>寻找一个超平面来将所有样本正确分开 (条件1)</li><li>并且保证超平面到两类样本的边界到超平面的距离和最大且相等 (条件2)</li></ul><p>条件一其实就是线性可分的条件，条件二是为了保证鲁棒性，保证两类样本到超平面的距离最大，就相当于保留了判断时的裕量，这样即使数据有噪声，只要噪声不是太过于离谱，都不会产生误判，而保证两类样本边界到超平面距离相等则是为了不偏向某一方。</p><p>用图来直观地感受下上面两条约束，svm模型图示如下：</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQud2lraW1lZGlhLm9yZy93aWtpcGVkaWEvY29tbW9ucy90aHVtYi83LzcyL1NWTV9tYXJnaW4ucG5nLzMwMHB4LVNWTV9tYXJnaW4ucG5n" alt="svm模型"></p><p>现在我们可以用数学的方式来描述这两个想法。</p><p>对于约束一，假设有一个超平面$w^Tx+b=0$可以将两类样本分开，那么对于正类样本$x_p$来说必然有$w^Tx_p+b &gt; 0$；同样地，对于负类样本$x_n$来说也会有$w^Tx_n + b &lt; 0$。如果我们让正类样本标签为$y_p=+1$而负类样本标签为$y_p=-1$，那么我们可以统一描述为</p><script type="math/tex; mode=display">y(w^Tx+b) > 0 \tag{1}</script><p>现在我们来看<strong>条件2</strong>。</p><p>为了对条件2进行建模，我们首先要求空间中任意一点到超平面$w^Tx+b=0$的距离。现在我们假设有一点$x_1$，它到超平面的距离$\gamma$可以写做</p><script type="math/tex; mode=display">\gamma=|w^Tx_1+b| \tag{2}</script><p>这个公式怎么来的呢？我们可以假设点$x_1$在超平面上的投影为$x_0$,那么从$x_1$指向$x_0$的这个向量就等于$x_1 - x_0$，同时这个向量的方向与超平面的法向量是一致的，我们把超平面化为$\frac{w^Tx+b}{||w||} = 0$,可以得到法向量$\frac{w}{||w||}$,所以我们可以得出</p><script type="math/tex; mode=display">\frac{\gamma w}{||w||} = x_1 - x_0 \tag{3}</script><p>其中$\gamma$就是点$x_1$到超平面的距离。式子(3)简单地做一下变形就可以得到式子(4)</p><script type="math/tex; mode=display">\frac{\gamma w^Tw}{||w||} = w^T(x_1 - x_0) + b - b = w^Tx_1 + b - (w^Tx_0 + b) \tag{4}</script><p>因为$x_0$是超平面$w^Tx + b = 0$上面的点，也就意味着$w^Tx_0 + b = 0$,这样如果只看距离的大小而不看方向的话，式子(4)就可以化为</p><script type="math/tex; mode=display">\gamma  = \frac{|w^Tx_1+b|}{||w||}</script><p>好了，有了距离公式之后我们可以计算一下两类样本到超平面的边界距离之和了。假设有正类的边界样本$x_+$和负类边界样本$x_-$, 距离之和为</p><script type="math/tex; mode=display">\gamma_+ + \gamma_- = \frac{w^T(x_+ - x_-)}{||w||} \tag{5}</script><p>我们观察一下式子(5)，在给定样本之后，由于分母$||w||$的存在，消除了参数向量$w$的<em>模长</em>的影响, $\frac{w}{||w||}$相当于一个方向与$w$同向的单位向量。于是我们只需要关注参数向量$w$的方向，而不需要关注其长度，从另一个角度来说，我们可以对$w$进行任意倍数的缩放而不会影响超平面。于是我们可以随意地令：</p><script type="math/tex; mode=display">    \begin{cases}        \quad w^T(x_+-x_-)=2 \\        \quad w^Tx_+ + b = 1 \\        \quad w^Tx_- + b = -1    \end{cases}</script><p>这并没有什么难以理解的，因为不管$w$和$b$的方向如何，我们总是可以对其进行缩放使得上述等式成立。这样由于正负类边界到超平面的距离都缩放到了$1$，式子(5)改写成</p><script type="math/tex; mode=display">\gamma_+ + \gamma_- = \frac{2}{||w||} \tag{6}</script><p>于是约束(1)也需要改写成</p><script type="math/tex; mode=display">y(w^Tx+b) \ge 1 \tag{7}</script><p>于是式子(6)和(7)构成了我们的优化目标与约束：</p><p>又由于最大化$\frac{2}{||w||}$与最小化$\frac{||w||^2}{2}$是等价的，而后者可以得到一个更规整的导数，方便后续处理，所以我们把上述的优化问题重写为:</p><script type="math/tex; mode=display">\begin{cases}    \quad \underset{w,b}{\arg \min} \frac{||w||^2}{2} \\    \quad s.t. \quad y_i(w^Tx_i + b) \ge 1, \quad i = (1, 2, 3, ...m)\end{cases} \tag{8}</script><p>其实我们重新审视一下式子(8), 约束条件未免太过严格，实际上大多数数据都很少存在这样完美的线性可分的条件，于是我们打算放宽一点限制: 不严格要求对每个样本都满足约束条件$y_i(w^Tx_i + b) \ge 1$，而是允许一定程度地违反该约束，并且违反程度通过$max(0, 1 - y_i(w^Tx_i+b))$来量化并且作为一个正则化项加入到优化目标当中。于是式子(8)进一步写成</p><script type="math/tex; mode=display">\underset{w,b}{\arg \min} \frac{||w||^2}{2} + C\sum_{i=1}^m{max(0, 1-y_i(w^Tx_i+b))} \tag{9}</script><p>现在看一看式子(9),很遗憾，由于含有非线性且不连续的部分，导致式子(9)也难以求解，于是我们只能寻求新的方法。好在这类问题前人们已经研究过了，解决的办法称之为:<strong>松弛变量法</strong>。在最优化领域，如果我们的约束函数全为’$\ge$’或者’$\le$’条件时，我们希望对满足约束的样本保持原有的约束，而对那些不满足约束的样本适当放松约束(边界变得松弛了)，并且将放松的程度作为一个惩罚量加入最优化目标函数中，这样我们就可以在更大的可行域中去最优化目标函数。在这里的做法是将约束$y_i(w^Tx_i+b) \ge 1$放松到$y_i(w^Tx_i + b) \ge 1-\xi_i$并且将$\xi_i$作为一个惩罚量加入目标函数中去，根据这个想法得到的优化目标与约束为</p><script type="math/tex; mode=display">\begin{aligned}    & \underset{w,b,\xi_i}{\arg \min} \frac{||w||^2}{2} + C\sum_{i=0}^m\xi_i \\    s.t.    & \begin{cases}        y_i(w^Tx_i + b) \ge 1-\xi_i, \quad i = (1, 2, 3...,m) \\        \xi_i \ge 0, \quad i = (i, 2, 3...,m)    \end{cases}\end{aligned} \tag{10}</script><p>以上式子就是SVM的带约束优化目标了，解决式$(10)$的方式有很多，一般通用的做法是通过拉格朗日乘子法将带约束问题转换为无约束的最优化问题，从而能够利用梯度下降，坐标上升法等迭代算法来求解。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;SVM-优化问题的导出&quot;&gt;&lt;a href=&quot;#SVM-优化问题的导出&quot; class=&quot;headerlink&quot; title=&quot;SVM - 优化问题的导出&quot;&gt;&lt;/a&gt;SVM - 优化问题的导出&lt;/h1&gt;&lt;p&gt;svm的想法其实非常朴素：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;寻找一
      
    
    </summary>
    
      <category term="机器学习" scheme="https://embolismsoil.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://embolismsoil.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="https://embolismsoil.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>在Qt中实现数字音频均衡器</title>
    <link href="https://embolismsoil.github.io/2019/07/12/%E5%9C%A8Qt%E4%B8%AD%E5%AE%9E%E7%8E%B0%E6%95%B0%E5%AD%97%E9%9F%B3%E9%A2%91%E5%9D%87%E8%A1%A1%E5%99%A8/"/>
    <id>https://embolismsoil.github.io/2019/07/12/在Qt中实现数字音频均衡器/</id>
    <published>2019-07-11T17:06:56.000Z</published>
    <updated>2019-07-12T18:42:37.362Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Qt实现数字音频均衡器"><a href="#Qt实现数字音频均衡器" class="headerlink" title="Qt实现数字音频均衡器"></a>Qt实现数字音频均衡器</h1><p>在实现音频播放器的时候，我们常常需要一个均衡器来调节各个频段的增益，就是我们平常说的调<strong>重低音</strong>。一个数字均衡器的架构通常都如图所示：<br><img src="filter.png" alt="数字均衡器"></p><p>从图中可以看到，这里的数字均衡器实际上就是三个滤波器，各个滤波器分别负责不同频段的音频调节，这三个滤波器叫做滤波器组。当然一个数字滤波器组也不一定只有三个滤波器，理论上来说可以有任意多个滤波器，而且滤波器越多，能调整的也就越精细。</p><p>从物理上来说人类听觉的频率范围在0~20kHZ这个范围，于是我们定义三个频段：[0,Hz 400Hz], [400, 2000HZ], [2000Hz, 无穷]，分别为低音，中音和高音。于是我们只需要设计出来一个[0, 400Hz]的低通滤波器，一个[400, 2000Hz]的带通滤波器，以及一个[2000Hz，无穷]的高通滤波器就可以组成一个均衡器了。当然本文并不是讨论如何设计滤波器的，这是一个复杂的数学推导过程，有兴趣的可以来信探讨。另外，我已经设计好了这三个滤波器，文末会附上代码。</p><p>好了，现在我们有了思路了，来看看如何在Qt中实现这个想法。首先最重要的是提供这个滤波器组，一个滤波器组大概长这样：</p><pre><code class="lang-cpp">class EQFilterGroup{public:    /*参数： 低音增益， 中音增益， 高音增益*/    EQFilterGroup(float const lowGain, float const midGain, float const highGain);    /*setter and getter here*/        virtual QBuffer* filter(QAudioBuffer const&amp; buffer);}</code></pre><p>这个类提供一个接口，这个接口输入一个音频帧(<code>QBuffer</code>)，然后输出滤波后的音频帧。</p><p>然后我们看一下Qt中如何播放一个音频流：</p><pre><code class="lang-cpp">      QFile sourceFile;      QAudioOutput *audio = new QAudioOutput(this);      sourceFile.setFileName(&quot;/tmp/test.raw&quot;);      sourceFile.open(QIODevice::ReadOnly);      QAudioFormat format;      format.setSampleRate(8000);      format.setChannelCount(1);      format.setSampleSize(8);      format.setCodec(&quot;audio/pcm&quot;);      format.setByteOrder(QAudioFormat::LittleEndian);      format.setSampleType(QAudioFormat::UnSignedInt);      QAudioDeviceInfo info(QAudioDeviceInfo::defaultOutputDevice());      if (!info.isFormatSupported(format)) {          qWarning() &lt;&lt; &quot;Raw audio format not supported by backend, cannot play audio.&quot;;          return;      }      audio = new QAudioOutput(format, this);      audio-&gt;start(&amp;sourceFile);</code></pre><p>其实非常简单，只需要给<code>QAudioOutput</code>这个组件设置好播放的音频格式参数，然后提供一个音频流就可以播放这个音频流了，在Qt里面所有的数据流都被抽象称为了<code>QIODevice</code>, 当然音频流也不例外，代码片段中的<code>sourceFile</code>就是一个音频文件数据流，它是<code>QIODevice</code>的子类。<br>于是现在的问题是： 我们如何将<code>EQFilterGroup</code>整合到上述代码当中？如果我们能把滤波器组伪装成一个像上面代码里<code>sourceFile</code>的文件一样就好了，那么<code>QAudioOutput</code>就可以直接读取滤波器组滤波后的数据流了。这里我们可以使用<a href="https://mp.csdn.net/mdeditor/50808990#" target="_blank" rel="noopener">适配器模式</a>来帮助我们完成这一个伪装。根据适配器模式的做法，我们只需要继承<code>QIODevice</code>然后实现对应的接口并且集成<code>EQFilterGroup</code>的滤波功能就可以实现一个<strong>可以滤波的<code>QIODevice</code> 了</strong>。大概代码如下：</p><pre><code class="lang-cpp">class AudioBufferDevice : public QIODevice{    Q_OBJECTpublic:    explicit AudioBufferDevice(QAudioDecoder *decoder, EQFilterGroup* filter, QObject *parent = nullptr);    virtual bool atEnd() const override;    virtual qint64 bytesAvailable() const override;protected:    virtual qint64 readData(char* data, qint64 size) override;    virtual qint64 writeData(const char *data, qint64 maxSize);private:    QAudioDecoder* _decoder;    QQueue&lt;QBuffer*&gt; _queue;    QQueue&lt;QAudioBuffer*&gt; _abuffer_queue;    EQFilterGroup* _filter;    bool _isFinished;};</code></pre><p>由于我们需要播放的是mp3文件，所以我们首先要通过<code>QAudioDecoder</code>来将mp3文件解码成音频帧，然后将音频帧输入滤波器组，滤波器组将滤波后的音频帧写入一个FIFO缓冲区内，并且通过<code>QIODevice::readData</code>接口向外界提供这些滤波后音频帧的数据流。当然，出于性能考虑，从<code>QAudioDecoder</code>解码到<code>EQFilterGroup</code>将滤波后数据写入缓冲池这一个过程也可以放入另一个线程中进行。</p><p>这样完成了上述的类之后，我们就可以实现一个<strong>低音炮</strong>播放器了：</p><pre><code class="lang-cpp">      EQFilterGroup* filter = new EQFilterGroup(2.0, 0.5, 0.5); //放大低音2倍, 中音高音弱化为1/2      QAudioDecoder* decoder = new QAudioDecoder(this);      decoder-&gt;setSourceFilename(&quot;/tmp/test.raw&quot;);      QAudioFormat format;      format.setSampleRate(8000);      format.setChannelCount(1);      format.setSampleSize(8);      format.setCodec(&quot;audio/pcm&quot;);      format.setByteOrder(QAudioFormat::LittleEndian);      format.setSampleType(QAudioFormat::UnSignedInt);      decoder-&gt;setAudioFormat(format);      QIODevice *device = new AudioBufferDevice(decoder, filter);      device-&gt;open(QIODevice::ReadOnly);      QAudioOutput *audio = new QAudioOutput(this);      QAudioDeviceInfo info(QAudioDeviceInfo::defaultOutputDevice());      if (!info.isFormatSupported(format)) {          qWarning() &lt;&lt; &quot;Raw audio format not supported by backend, cannot play audio.&quot;;          return;      }      audio = new QAudioOutput(format, this);      audio-&gt;start(device);</code></pre><p>以上就是在Qt中实现数字音频均衡器的全部啦，完整的代码在这里<a href="https://github.com/EmbolismSoil/EQFilterGroup" target="_blank" rel="noopener">Qt实现数字音频均衡器</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Qt实现数字音频均衡器&quot;&gt;&lt;a href=&quot;#Qt实现数字音频均衡器&quot; class=&quot;headerlink&quot; title=&quot;Qt实现数字音频均衡器&quot;&gt;&lt;/a&gt;Qt实现数字音频均衡器&lt;/h1&gt;&lt;p&gt;在实现音频播放器的时候，我们常常需要一个均衡器来调节各个频段的增益，
      
    
    </summary>
    
      <category term="Qt" scheme="https://embolismsoil.github.io/categories/Qt/"/>
    
    
      <category term="C++" scheme="https://embolismsoil.github.io/tags/C/"/>
    
      <category term="Qt" scheme="https://embolismsoil.github.io/tags/Qt/"/>
    
      <category term="音频算法" scheme="https://embolismsoil.github.io/tags/%E9%9F%B3%E9%A2%91%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
